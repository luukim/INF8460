{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## École Polytechnique de Montréal\n",
    "## Département Génie Informatique et Génie Logiciel\n",
    "\n",
    "## INF8460 – Traitement automatique de la langue naturelle - TP1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectifs d'apprentissage: \n",
    "\n",
    "•\tSavoir accéder à un corpus, le nettoyer et effectuer divers pré-traitements sur les données\n",
    "•\tSavoir effectuer une classification automatique des textes pour l’analyse de sentiments\n",
    "•\tEvaluer l’impact des pré-traitements sur les résultats obtenus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Équipe et contributions \n",
    "Veuillez indiquer la contribution effective de chaque membre de l'équipe en pourcentage et en indiquant les modules ou questions sur lesquelles chaque membre a travaillé\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nom Étudiant 1: Luu Thien-Kim (1834378) 33.33%\n",
    "\n",
    "Nom Étudiant 2: Mellouk Souhaila (1835144) 33.33%\n",
    "\n",
    "Nom Étudiant 3: Younes Mourad (1832387) 33.33%\n",
    "\n",
    "Nous avons tous travaillé ensemble sur chaque question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Librairies externes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from typing import List, Literal, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Valeurs globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_path = \"data\"\n",
    "output_path = \"output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def read_data(path: str) -> Tuple[List[str], List[bool], List[Literal[\"M\", \"W\"]]]:\n",
    "    data = pd.read_csv(path)\n",
    "    inputs = data[\"response_text\"].tolist()\n",
    "    labels = (data[\"sentiment\"] == \"Positive\").tolist()\n",
    "    gender = data[\"op_gender\"].tolist()\n",
    "    return inputs, labels, gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_data = read_data(os.path.join(data_path, \"train.csv\"))\n",
    "test_data = read_data(os.path.join(data_path, \"test.csv\"))\n",
    "\n",
    "train_data = ([text.lower() for text in train_data[0]], train_data[1], train_data[2])\n",
    "test_data = ([text.lower() for text in test_data[0]], test_data[1], test_data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentiment(data):\n",
    "    sentimentList = []\n",
    "    for sentiment in data:\n",
    "        if sentiment:\n",
    "            sentimentList.append(\"Positive\")\n",
    "        else:\n",
    "            sentimentList.append(\"Negative\")\n",
    "    return sentimentList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pré-traitement et Exploration des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Lecture et prétraitement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Dans cette section, vous devez compléter la fonction preprocess_corpus qui doit être appelée sur les fichiers train.csv et test.csv. La fonction preprocess_corpus appellera les différentes fonctions créées ci-dessous. Les différents fichiers de sortie doivent se retrouver dans le répertoire output.  Chacune des sous-questions suivantes devraient être une ou plusieurs fonctions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join(data_path, \"train.csv\")\n",
    "test_path = os.path.join(data_path, \"test.csv\")\n",
    "\n",
    "train_phrases_path = os.path.join(output_path, \"train_phrases.csv\")\n",
    "test_phrases_path = os.path.join(output_path, \"test_phrases.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 1) Segmentez chaque corpus en phrases, et stockez-les dans un fichier `nomcorpus`_phrases.csv (une phrase par ligne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/mouradyounes/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/mouradyounes/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\") \n",
    "nltk.download(\"wordnet\")\n",
    "import csv\n",
    "\n",
    "def segmentSentences(path) :\n",
    "    data = read_data(path)\n",
    "    corpus = data[0]\n",
    "    sentiment = getSentiment(data[1])\n",
    "    if not os.path.isdir(output_path) :\n",
    "        try:\n",
    "            os.mkdir(output_path)\n",
    "        except OSError:\n",
    "            print (\"Creation of the directory %s failed\" % path)\n",
    "        else:\n",
    "            print (\"Successfully created the directory %s \" % path)\n",
    "    newFilePath = output_path + '/' + os.path.splitext(os.path.basename(path))[0] + \"_phrases.csv\"\n",
    "    file = open(newFilePath, \"w\")\n",
    "    with open(newFilePath, \"w\") as f: \n",
    "        f.write(\"response_text\" + ',' + \"sentiment\" + ',' + \"op_gender\" +'\\n')\n",
    "        for i in range(len(corpus)) :\n",
    "            sentences = nltk.sent_tokenize(corpus[i])\n",
    "            for sentence in sentences:\n",
    "                sentence = sentence.replace('\"', '\"\"').replace('\"', '\"\"')\n",
    "                f.write('\"'+ sentence +'\"' + ',' + '\"' +sentiment[i]+ '\"'+ ',' + '\"'+data[2][i] + '\"\\n')\n",
    "                \n",
    "    return newFilePath\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output/test_phrases.csv'"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segmentSentences(train_path)\n",
    "segmentSentences(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 2) Normalisez chaque corpus au moyen d’expressions régulières en annotant les négations avec _Neg L’annotation de la négation doit ajouter un suffixe _NEG à chaque mot qui apparait entre une négation et un signe de ponctuation qui identifie une clause. Exemple : \n",
    "No one enjoys it.  no one_NEG enjoys_NEG it_NEG .\n",
    "\n",
    "I don’t think I will enjoy it, but I might.  i don’t think_NEG i_NEG will_NEG enjoy_NEG it_NEG, but i might."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPath(path) :\n",
    "    if \"train\" in path :\n",
    "        path = train_path\n",
    "    elif \"test\" in path :\n",
    "        path = test_path\n",
    "        \n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalize(path) :\n",
    "    with open(path, \"r\") as f :\n",
    "        data = list(f)\n",
    "    \n",
    "    newFilePath = output_path + '/' + os.path.splitext(os.path.basename(getPath(path)))[0] + \"_negation.csv\"\n",
    "    file = open(newFilePath, \"w\")\n",
    "    with open(newFilePath, \"w\") as f:\n",
    "        for sentence in data:\n",
    "            match = re.sub(\"(?i)(?<=not |n't | no )(.*?[,.(?!;]+)\", lambda m: m.group(1).replace(\" \", \"_NEG \")\n",
    "                           .replace(\".\", \"_NEG.\").replace(\",\", \"_NEG,\").replace(\"?\", \"_NEG?\").replace(\"!\", \"_NEG!\")\n",
    "                           .replace(\"(\", \"_NEG(\").replace(\";\", \"_NEG;\"), sentence)\n",
    "            f.write(match)\n",
    "            \n",
    "    return newFilePath\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output/test_negation.csv'"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize(train_phrases_path)\n",
    "normalize(test_phrases_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 3) Segmentez chaque phrase en mots (tokenisation) et stockez-les dans un fichier `nomcorpus`_mots.csv. (Une phrase par ligne, chaque token séparé par un espace, il n’est pas nécessaire de stocker la phrase non segmentée ici) ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def tokenize(path) :\n",
    "    sentences = []\n",
    "    \n",
    "    data = read_data(path)\n",
    "    corpus = data[0]        \n",
    "    sentiment = getSentiment(data[1])\n",
    "    newFilePath = output_path + '/' + os.path.splitext(os.path.basename(getPath(path)))[0] + \"_mots.csv\"\n",
    "    file = open(newFilePath, \"w\")\n",
    "    with open(newFilePath, \"w\") as f: \n",
    "        f.write(\"response_text\" + ',' + \"sentiment\" + ',' + \"op_gender\" +'\\n')\n",
    "        for i in range(len(corpus)) :\n",
    "            listTokens = nltk.word_tokenize(corpus[i])\n",
    "            tokens = ' '.join(listTokens)\n",
    "            f.write('\"' + tokens + '\"' + ',' + '\"' + sentiment[i]+ '\"'+ ',' + '\"' + data[2][i] + '\"\\n')\n",
    "                \n",
    "    return newFilePath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize(train_phrases_path)\n",
    "tokenize(test_phrases_path)\n",
    "\n",
    "train_mots_path = os.path.join(output_path, \"train_mots.csv\")\n",
    "test_mots_path = os.path.join(output_path, \"test_mots.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 4) Lemmatisez les mots et stockez les lemmes dans un fichier `nomcorpus`_lemmes.csv (une phrase par ligne, les lemmes séparés par un espace) ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def lemmatize(path) :\n",
    "    with open(path, \"r\") as f :\n",
    "        data = list(f)\n",
    "        \n",
    "    newFilePath = output_path + '/' + os.path.splitext(os.path.basename(getPath(path)))[0] + \"_lemmes.csv\"\n",
    "    lemmzer = nltk.WordNetLemmatizer()\n",
    "    \n",
    "    file = open(newFilePath, \"w\")\n",
    "    with open(newFilePath, \"w\") as f: \n",
    "        for sentences in data :\n",
    "            lemmes = [lemmzer.lemmatize(token) for token in sentences.split()]\n",
    "            sentences = ' '.join(lemmes)\n",
    "            f.write(sentences+'\\n')\n",
    "                \n",
    "    return newFilePath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output/test_lemmes.csv'"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize(train_mots_path)\n",
    "lemmatize(test_mots_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 5) Retrouvez la racine des mots (stemming) en utilisant nltk.PorterStemmer(). Stockez-les dans un fichier `nomcorpus`_stems.csv (une phrase par ligne, les racines séparées par une espace) ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def stemmize(path) :    \n",
    "\n",
    "    data = read_data(path)\n",
    "    corpus = data[0]        \n",
    "    sentiment = getSentiment(data[1])\n",
    "        \n",
    "    path = getPath(path)\n",
    "    newFilePath = output_path + '/' + os.path.splitext(os.path.basename(path))[0] + \"_stems.csv\"\n",
    "    \n",
    "    stemmer = nltk.PorterStemmer()\n",
    "    \n",
    "    file = open(newFilePath, \"w\")\n",
    "    with open(newFilePath, \"w\") as f:\n",
    "        f.write(\"response_text\" + ',' + \"sentiment\" + ',' + \"op_gender\" +'\\n')\n",
    "        for i in range(len(corpus)) :\n",
    "            stems = [stemmer.stem(token) for token in corpus[i].split()]\n",
    "            sentences = ' '.join(stems)\n",
    "            f.write('\"' + sentences + '\"' + ',' + '\"' + sentiment[i]+ '\"'+ ',' + '\"' + data[2][i] + '\"\\n')\n",
    "                \n",
    "    return newFilePath\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output/test_stems.csv'"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmize(train_mots_path)\n",
    "stemmize(test_mots_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 6) Ecrivez une fonction qui supprime les mots outils (stopwords) du corpus. Vous devez utiliser la liste de stopwords de NLTK ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mouradyounes/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words(\"english\")\n",
    "\n",
    "def deleteStopWords(path) :\n",
    "\n",
    "    data = read_data(path)\n",
    "    corpus = data[0]        \n",
    "    sentiment = getSentiment(data[1])\n",
    "        \n",
    "    path = getPath(path)\n",
    "    newFilePath = output_path + '/' + os.path.splitext(os.path.basename(path))[0] + \"_stopWords.csv\"\n",
    "    stopwords_english = set(stopwords.words(\"english\"))\n",
    "    output = []\n",
    "    \n",
    "    file = open(newFilePath, \"w\")\n",
    "    with open(newFilePath, \"w\") as f: \n",
    "        f.write(\"response_text\" + ',' + \"sentiment\" + ',' + \"op_gender\" +'\\n')\n",
    "        for i in range(len(corpus)) :\n",
    "                newSentence = [token for token in nltk.word_tokenize(corpus[i]) if token not in stopwords_english]\n",
    "                sentences = ' '.join(newSentence)\n",
    "                output.append(sentences)\n",
    "                f.write('\"' + sentences + '\"' + ',' + '\"' + sentiment[i]+ '\"'+ ',' + '\"' + data[2][i] + '\"\\n')\n",
    "                \n",
    "    return output\n",
    "                \n",
    "#enlever la création de nouveaux fichiers\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thanks back !',\n",
       " 'Yep , University Alberta .',\n",
       " 'You live around ?',\n",
       " \"please n't sell land Steve\",\n",
       " 'shaking head ignorance deliberate ignoring facts FDR , Pearl Harbor , WWII .',\n",
       " 'To contemplated tri , perhaps ?',\n",
       " 'Pshh ... Is treat props .. Just go around deleting ? ! ? !',\n",
       " 'Sureeeeeeeeeeeeeeeeeeee I see .',\n",
       " \": pYeah 's definitely still bugs around .\",\n",
       " \"My workout last night posted today 's date .\",\n",
       " 'lol',\n",
       " 'Thanks !',\n",
       " 'I also love bacon .',\n",
       " ': )',\n",
       " 'Hello Isaac !',\n",
       " \"My copy arrived yesterday France , I 'm happy excited read ! ! !\",\n",
       " 'XD',\n",
       " 'We need keep Bob Menendez congress .',\n",
       " \"It 's really excellent lecture , I believe her.so , fake till make !\",\n",
       " \"I 'm human according questions .\",\n",
       " 'And tone great , I lot fun : )',\n",
       " \"You 're awesome ! !\",\n",
       " '!',\n",
       " 'B Mattek awesome , I love bad girl risque style .',\n",
       " \"Brit : I 'm glad u r Fox tonight .\",\n",
       " 'You calm people bring common sense .',\n",
       " 'You class act !',\n",
       " '!',\n",
       " 'Good luck .',\n",
       " 'I really hope make .',\n",
       " 'Please reach Mr. Trump !',\n",
       " 'Thanks feedback !',\n",
       " 'Ill sure check phrasing next time : )',\n",
       " 'This talk powerful ..',\n",
       " 'Currently enjoying Ashley Bell .',\n",
       " 'Love writing much Dean .',\n",
       " 'Thank gifts give us',\n",
       " 'carpet bagging troll ... Liz Warren represent Massachusetts',\n",
       " 'I like much .',\n",
       " \"It 's great area explore 's long drive ... good kids .\",\n",
       " \"Monotasking ... god , mean actually good something , 's world coming .\",\n",
       " \"Dont know 's hype sells nowadays , skill nothing anymore , dinosaur .\",\n",
       " 'Explain guys continue letting Obama break laws .',\n",
       " 'Lawlessness abounds D.C. Where checks balances ?',\n",
       " 'Arrest .',\n",
       " 'Proper rest important , bad call ....',\n",
       " 'OOOOOHHHHH , thanks !',\n",
       " 'Paula fans !',\n",
       " 'Like page !',\n",
       " 'We Love You Paula',\n",
       " 'Seems bit disingenuous Seth , find different issue !',\n",
       " 'This wonderful .',\n",
       " 'Thank much putting !',\n",
       " \"Megyn , great job I bet 're wonderful lady person !\",\n",
       " 'RUN , CLAIRE , RUN ! ! ! ! !',\n",
       " 'WE NEED YOU ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !',\n",
       " '!',\n",
       " 'You bully .',\n",
       " 'Still insulting 12 hours later .',\n",
       " 'Seriously , wrong ?',\n",
       " 'You need professional help .',\n",
       " 'A little death scare amazing things motivation .',\n",
       " 'Thanks following back .',\n",
       " 'Great PP !',\n",
       " 'Hi !',\n",
       " 'I heard TED story .',\n",
       " 'Your story useful , teach success failure syndrome .',\n",
       " 'everything get syndrome .',\n",
       " 'Were happy end ?',\n",
       " 'Then grand old idea : )',\n",
       " 'I never known anyone stand platform lies long without falling pit disgrace .',\n",
       " 'Just ask Sarah Palin , Glenn Beck , David Vitter .',\n",
       " 'We federal transportation funding potentially great Trans-Hudson Passenger Rail Tunnel , governor paid debts .',\n",
       " \"Got ta keep Harper high rotation , 's boy !\",\n",
       " 'Dooooo eeeet .',\n",
       " \"I n't seen pup yet luckbox : )\",\n",
       " '..... thank Jill ...',\n",
       " \"Just got phone call daughter one granddaughter sitting Hanabusa booth Maku ' u Farmers market near Pahoa .\",\n",
       " 'Hooray cell phones !',\n",
       " 'Fox noise never report good news .',\n",
       " 'If failing would run breaking news .',\n",
       " 'Yay !',\n",
       " 'boobs proportionate hands , everyone gets wi ...',\n",
       " 'Proud lucky Mike Capuano Congressman !',\n",
       " 'Thank great work !',\n",
       " \"Hey , daddy knows ... And I guess ai n't saying .\",\n",
       " 'No prob .',\n",
       " 'Always glad help',\n",
       " 'Safe ?',\n",
       " \"Apparently illegal aliens since voted Obama 's amnesty .\",\n",
       " 'Dont forget Chogall high 20s .',\n",
       " 'pleasure !',\n",
       " 'keep good work !',\n",
       " ': )',\n",
       " \"Do n't let crazy anti-health care reformers get Brian .\",\n",
       " \"We 're counting !\",\n",
       " 'Enjoyed presentation Dave .',\n",
       " 'Any suggestions related reading material ?',\n",
       " 'Un fuerte abrazo tu mama Ruben , que reciba muchas bendiciones hoy siempre .',\n",
       " 'My friends I also look forward voting US Senate !',\n",
       " 'Thanks .',\n",
       " 'And interesting variety well !',\n",
       " 'Dont worry , Ive googled shed load listened couple peoples advice .',\n",
       " 'I think Ill ok .',\n",
       " 'Now Im picturing Chicken assassin , poisoning tea ... maybe french accent .',\n",
       " 'Contains graphic artistic nudity strong sexual references .',\n",
       " 'Not suitable young audiences .',\n",
       " 'I actually shot Tofino , break surf .',\n",
       " 'Thanks following back !',\n",
       " 'Never heard guy till .',\n",
       " 'He jam !',\n",
       " 'way win big florida .... !',\n",
       " 'lol',\n",
       " 'Rebooting !',\n",
       " 'Week 1 .',\n",
       " 'Kara great !',\n",
       " 'More brainey women !',\n",
       " 'Very nice Congress-man GW Meeks',\n",
       " 'There needs Jackie signs ! ! !',\n",
       " 'Too many loser Joe Bock signs around .',\n",
       " \"He 's racist anti-Jew loser !\",\n",
       " 'http : //freebeacon.com/politics/scholarship-at-palestinian-university-named-after-dem-congressional-candidate/ [ [ SHARE ] ]',\n",
       " 'oh lala ... race !',\n",
       " ': D',\n",
       " 'The inference dynamic processing helps visual integration develop impressed .',\n",
       " 'Some talk twitter Gove might get defence .',\n",
       " 'God help us thats true',\n",
       " \"At least 're back !\",\n",
       " 'situations china complex , one fix',\n",
       " 'Heavenly Father , Please Heal Our Land .....',\n",
       " 'Ha !',\n",
       " \"I saw LOL 'd I started looking feed learned great exercises lifting bootay !\",\n",
       " 'I also saw men group thought would get kick !',\n",
       " \"Wow looks like I 'm last stage declination ...\",\n",
       " \"I 'm 16 ..\",\n",
       " 'Better pick Arabic ASAP : )',\n",
       " \"The fact kind problem cropping engineering thousands years development makes glad I n't work field !\",\n",
       " \"Imagine 've spent years designing building bridge new , quite unknown issue arises opening day - poor engineers !\",\n",
       " 'Good morning Dunia que tengan un excelente dia',\n",
       " 'Keep good work !',\n",
       " 'Thank !',\n",
       " '!',\n",
       " \"Lol I 'll make looks great always\",\n",
       " 'precious family pic .',\n",
       " 'Dont ever give Trust Moogles Exdeath , TMR bad .',\n",
       " 'Unless using Orlandu , Cecils important .',\n",
       " 'Do options Fryevia ?',\n",
       " 'Loved book !',\n",
       " \"In fact , reading cover cover one 'go ' , I began reading !\",\n",
       " 'I think book , message , ending .... could change lives .',\n",
       " '@ Dean Koontz .',\n",
       " 'In clip interview ... library shelves ?',\n",
       " 'Thanks well .',\n",
       " 'Look forward seeing feeds !',\n",
       " ': )',\n",
       " 'But lives parents ?',\n",
       " 'Hopefully didnt use credit card .',\n",
       " 'Welcome group !',\n",
       " 'My nose bench 230 , whats .',\n",
       " 'This talk made feel sorry us humans trying move sexily club bed .',\n",
       " 'So laborious !',\n",
       " \"We discuss 's fair divide pie , anyone accepts position paid multiple times average nothing discuss , bring art justification table .\",\n",
       " 'Those super-rich necessary .',\n",
       " 'I strange feeling I watching professional preacher New York needs urgent psychiatric treatment ( lot cash ) !',\n",
       " \": ) In case anybody knows `` `` change , whether political , social personal `` `` Mallika 's `` `` dance `` `` actually produced , share us .\",\n",
       " 'WATCH MORE TALK WHILE nothing IS ACTUALLY BEING DONE ! ! ! ! !',\n",
       " 'NO THANKS ! ! !',\n",
       " '!',\n",
       " 'And stop posting ? ?',\n",
       " '?',\n",
       " 'Senator DeSantis , sounds good ! !',\n",
       " 'I hate lose district though .',\n",
       " 'Come show Cali show asian girls ucla done !',\n",
       " \"Looks like obama smelled Putin 's fartski\",\n",
       " 'Hey Mark - still around ?',\n",
       " 'Good .',\n",
       " 'Thanks follow man',\n",
       " 'Yes , A trail around ADK park towns linked together , The kids youth conservation corp could use work .',\n",
       " 'The trails already .',\n",
       " \"That 's awesome !\",\n",
       " 'We lot common : )',\n",
       " 'I really wish still Dean Koontz gift store still open , I need Pico Mundo Grille coffee mugs !',\n",
       " 'thanks follwing back man !',\n",
       " 'My dated talk gotten .',\n",
       " \"If price long-distance n't zero talking India would prohibitively expensive .\",\n",
       " 'Chris Anderson really needs update video talk something geared toward 2011',\n",
       " 'Or * * * * likely ingress portals , put bored workers .',\n",
       " 'Edited emphasize',\n",
       " 'That clip makes want re-watch whole series ... You ready The Breakfast tomorrow morning ?',\n",
       " 'I love field hockey .',\n",
       " 'I played football field hockey favorite sport softball girls basketball',\n",
       " 'My pleasure earned Morgan !',\n",
       " '!',\n",
       " 'Thank Lucandebulter !',\n",
       " 'I actually kind like !',\n",
       " \"Go Get 'Em Rand Paul .\",\n",
       " 'I heard Laura Ingraham show today great !',\n",
       " 'Glad see front making case .',\n",
       " 'guess I voted ! HA !',\n",
       " '!',\n",
       " 'Spreading love !',\n",
       " 'Thank much needed inspiration !',\n",
       " 'She good speaker topic .',\n",
       " 'We close age .',\n",
       " 'Thank speaking .',\n",
       " \"I 'm glad , great role model others overcoming shame .\",\n",
       " \"I imagine flooded people 's feeds positive comments , would wonderful .\",\n",
       " 'Thank much citizen Salisbury .',\n",
       " 'I enjoy listening enlightening words Professor Dawkins .',\n",
       " 'I always expressed atheism openly .',\n",
       " 'We atheists militant grave hindrance religion poses science .',\n",
       " 'We must least even playing field .',\n",
       " 'They want transparency !',\n",
       " 'Plain simple !',\n",
       " 'Oh worries , I thought .',\n",
       " 'And PP brings smile face every time I see !',\n",
       " 'Awesome article Eric ! !',\n",
       " 'So happy !',\n",
       " '!',\n",
       " 'fascinating - really enjoyed talk',\n",
       " 'Thank !',\n",
       " \"If need anything , I 'm ^^\",\n",
       " 'Sorry old hat ; really suddenly ( ) discovered people need valued ! ?',\n",
       " \"Some 'old guy ' ... Ahh , Maslow ! ! !\",\n",
       " 'I wish someone would pay reinvent wheel !',\n",
       " 'Thanks follow back haha : )',\n",
       " 'Never heard speak rights Muslim women',\n",
       " 'It victory uninsured reason .',\n",
       " 'Let us hope present law improved necessary changes .',\n",
       " 'Bitsie may beautiful place planet .',\n",
       " 'The water clear , lake rocks like nowhere else .',\n",
       " 'America need another $ 1 trillion + bankrupt America',\n",
       " 'Been inconsistent back track ! Thanks follow',\n",
       " 'Thanks hard work issue .',\n",
       " 'Most volume control T.V .',\n",
       " 'ads .',\n",
       " 'make difference annoyance broadcasting .',\n",
       " 'south mpls taxes going huge surplus ! ! !',\n",
       " '!',\n",
       " 'Easy stop sending money foreign nations start spending citizens foreign aide BS ! !',\n",
       " '!',\n",
       " 'Heroes leaders emerge .',\n",
       " 'Thank , Monica taking back narrative uplifting us thoughtful words , analysis decree empathy , compassion world public shaming click past .',\n",
       " 'luv u much tyra alwayas watch ANTM : ALL STARS',\n",
       " 'Shelley vote .',\n",
       " 'Jim Ballard',\n",
       " 'Your radiating joy .',\n",
       " 'See future cruise Alonzo .',\n",
       " 'Greatful All Americans vote Trump .',\n",
       " \"Awkwafina 's Genius ! ! !\",\n",
       " '!',\n",
       " 'Hi Claire !',\n",
       " 'Saw bus go downtown Columbia yesterday .',\n",
       " 'Was going try stop say hello ( I used Win Horner ... ) thank service .',\n",
       " 'You definitely vote !',\n",
       " 'lol .',\n",
       " 'im protein shake .',\n",
       " 'still got creatine though , eating loads tuna lol .',\n",
       " 'http : //twitchy.com/2015/05/10/watch-mark-halperins-ghastly-cuban-authenticity-interrogation-of-ted-cruz-video/',\n",
       " 'Thank much !',\n",
       " 'Actually , I situation like I confidence school I shy communicate others .',\n",
       " 'But I hopes I make watching !',\n",
       " 'In contrast , simple flat tax would flatten tax brackets , allow everyone pay comparable percentage tax , still hinder stimulative effect economic activity sales jobs increasing sales taxes would negatively impact .',\n",
       " 'Thanks back !',\n",
       " 'Great lifts !',\n",
       " ': )',\n",
       " 'It changed life : )',\n",
       " 'I feel much better knowing - Im alone .',\n",
       " 'True .',\n",
       " 'ES takes practice though .',\n",
       " 'If youre used timing maximum impact teamfight , going get value .',\n",
       " 'Medusa doesnt change part playstyle .',\n",
       " 'Its free value .',\n",
       " 'Great game , well played !',\n",
       " 'Early night !',\n",
       " 'I suggest reading theory created Allan Kardec .',\n",
       " 'He initiator research seeks present reasoned faith evolution spirit , proving reducing violence decreases proportionally evolution spirit time .',\n",
       " 'I LOVE YOU DAN EBERLE !',\n",
       " 'This wonderful -- thank much beautiful tribute .',\n",
       " 'She true , one big connected energy chosen manifest , beautiful part artists manifest .',\n",
       " 'We put Markwayne fight corruption .',\n",
       " \"Do n't let us .\",\n",
       " 'No problem !',\n",
       " 'Haha Thank youuuu !',\n",
       " ': D',\n",
       " 'something like `` `` best , rest come `` ``',\n",
       " 'Awesome ! !',\n",
       " 'What great honour !',\n",
       " \"My pleasure - keep keepin ' , brother .\",\n",
       " 'Life funny way throwing obstacles - adapt survive .',\n",
       " 'Very inspiring project ... As Pediatric Physical Therapist , I wondering children evalutated treated gross fine motor deficits .',\n",
       " 'Children rely visual/perceptual skills develop motorically .',\n",
       " 'We love watching .',\n",
       " 'You earth pleasant listen .',\n",
       " 'You dressed well tonight .',\n",
       " 'Do pay attention others jealous !',\n",
       " 'Good luck , hope works : - )',\n",
       " 'lot dopes vote clowns like Udall',\n",
       " 'Not entirely surprising given one State Senators powered solely hot air ?',\n",
       " 'shame , I actually havent listened albums full yet .',\n",
       " 'My internet crud watch youtube .',\n",
       " 'I got money buy .',\n",
       " 'Im working .',\n",
       " 'slowly .',\n",
       " 'I listened large majority songs , I wouldnt able name album , probably .',\n",
       " 'Wow Amy , moving !',\n",
       " 'You really inspired !',\n",
       " \"I 'd love translate increase awareness .\",\n",
       " 'Possibilities Dutch French .',\n",
       " 'Love talk !',\n",
       " 'Monday goodness begins !',\n",
       " 'thanks remembering !',\n",
       " 'Hope well !',\n",
       " 'Hey agree Harvey .',\n",
       " 'If tamper DNA make us physically powerful ?',\n",
       " 'Why tamper Psyche make us not-selfish , not-greedy , non-racist , non-terrorist , non-war ish , non-explotish ? ? ? ? ?',\n",
       " 'Suggestions welcome',\n",
       " 'Editing fucking point',\n",
       " 'thanks !',\n",
       " 'joined group',\n",
       " 'Mrs. Carter spoke thing ( Bunchy Carter Mama ) ... God Bless community ...',\n",
       " 'Goodmorning .',\n",
       " 'And good afternoon Netherlands ....',\n",
       " 'Good morning Chris .',\n",
       " 'Watching Berman & Romans `` `` Early Start `` `` waiting # NewDay ... Pre-Cubs game nerves .',\n",
       " 'Beautiful artowrk indeed ... must difficult craft sure .',\n",
       " '< href= `` `` http : //www.ultimatedefensesystem.com/ `` `` > Self Defense Moves < /a >',\n",
       " 'Hey man , could attend deaf hard hearing meet .',\n",
       " 'Could fun',\n",
       " 'Inmates need stick license plates , manufacturing anything detrimental saving lives .',\n",
       " 'Denny Crane shit',\n",
       " \"His talk reminds Edward De Bono 's book - `` `` Simplicity `` `` .\",\n",
       " 'Therefore I suggest 4th quadrant referred `` `` simplicity `` `` .',\n",
       " 'Thank much , Debbie ! !',\n",
       " \"You one amazing lady I 'm excited involved UAW politics !\",\n",
       " '!',\n",
       " \"Best talk I 've ever seen ; ) Even wathing talk makes feel really powerful .\",\n",
       " 'Thank sharing !',\n",
       " 'que hermosa may como sienpre',\n",
       " 'Thanks making effort coming support athletes !',\n",
       " 'Great work clear victory tonight !',\n",
       " 'If stretch , track !',\n",
       " ': )',\n",
       " 'I love -- two look like celebrities .',\n",
       " 'People magazine come .',\n",
       " 'make luck around hurr .',\n",
       " 'got shit',\n",
       " 'What ?',\n",
       " 'No mickey Ds Royale cheese ?',\n",
       " 'Honestly , class haha !',\n",
       " 'Bon chance mes amis !',\n",
       " 'I connect better Im staring words .',\n",
       " '... And It helps discern Sara ( Tinder ) Im talking ?',\n",
       " 'Edit : Im kidding , damn',\n",
       " 'That toxic masculinity strikes , evil evil men .',\n",
       " '< link >',\n",
       " 'Great clip Traci ... n rip Sam Kinison .',\n",
       " ': - ( True genius , ..',\n",
       " 'Very exciting .',\n",
       " 'I wish Tennis Channel would show mixed doubles matches',\n",
       " 'Rino ego size ass ..... I glad liberals ripping jackass .',\n",
       " \"I n't want running prez anyway .... gasbag .\",\n",
       " 'Egory Mullins polite .',\n",
       " 'Seriously ... One funniest tv shows EVER ! ! !',\n",
       " '!',\n",
       " \"sad still talking I 'm almost 65 !\",\n",
       " 'Thank showing astonishing animals ! ! !',\n",
       " 'It amazing observe animals , It requires special highly developed cells convert appearance skin like enviroment .',\n",
       " 'Just Unbelievable .',\n",
       " 'The words I listen video differences civilization , China ability hold differences lot civilization , So worry rise China .',\n",
       " 'It benefit world .',\n",
       " 'So Chinatown support jobs ?',\n",
       " 'I think everyone always mood Kimi win',\n",
       " 'You know whats also addictive Television ?',\n",
       " 'Reddit !',\n",
       " \"n't dismiss .\",\n",
       " 'see earlier comment ( April 4 )',\n",
       " 'I always liked .',\n",
       " 'It made could talk whatever wanted , opinions know agreed upon .',\n",
       " 'No isnt , update goes live time every single , clutter .',\n",
       " 'There wasnt one yesterday , update , post like removed .',\n",
       " 'Its necessary .',\n",
       " 'Also , I would like point NOWHERE post made topic say update live .',\n",
       " 'Thanks much Dave !',\n",
       " \"I really appreciate support : ) Guess I 'd better get run ... Hope great weekend !\",\n",
       " 'Im Canadian wings fan hes favourite Canadian player .',\n",
       " 'I , , feel way : P',\n",
       " 'Senator Wicker , Thank much voting earmarks ! ! !',\n",
       " 'Keep good work .',\n",
       " 'FYI Mr. Rivera , LOVED Eastwood , thank much .',\n",
       " 'Lets Mary champion',\n",
       " 'No surprise .... best person us ! ! ! ! ! !',\n",
       " '!',\n",
       " 'Right money sir tell Obama Reid money left spend waste .',\n",
       " 'The acceptable answer `` `` Yes `` `` , case wondering .',\n",
       " 'Great job Trevin Weech !',\n",
       " \"The CFYMCA , Teen board , proud team , sending nothing well wishes 've completed High School !\",\n",
       " 'Thank support , Congresswoman Tsongas !',\n",
       " 'Easy remember passwords always insecure guessed easily people',\n",
       " 'But [ assuming Ireland ] legally pay less per hour 16-18 years olds ... less full adults whove worked 2+ jobs past .',\n",
       " 'I learned tell story pictures .',\n",
       " 'I use create dramatic , emotional photos',\n",
       " 'Hehe , I know humor anyway : p',\n",
       " 'You welcome .',\n",
       " 'I propping workouts achievements , earned every one .',\n",
       " ': - )',\n",
       " '35,000 people die hunger day .',\n",
       " 'They probably need help penguins .',\n",
       " 'Absolutely vote Tom Cotton Asa Hutchinson I vote person I think would best person job elected',\n",
       " 'An idea worth spreading , man worth knowing .',\n",
       " 'Rock , friend ; rock .',\n",
       " 'Any one ever watch Zoom ?',\n",
       " 'Boston Mass !',\n",
       " 'Oh two one three foooooooooooooouuuur ! !',\n",
       " '!',\n",
       " 'wan na see least one intimate scene diesel katee',\n",
       " 'LOL great song !',\n",
       " \"I 've finding rock strength training station pandora excellent choice workouts .\",\n",
       " 'Osceola ; home Bartle Scout Reservation ; great memories .',\n",
       " \"Liked response today 's News Miner regarding opponent 's negative campaigning .\",\n",
       " 'thank : ) niceness , I went found something else prop !',\n",
       " 'This talk really inspiring me.You teach read books different ways.I hope help later life.I proud !',\n",
       " '!',\n",
       " 'Growing kids need real food , junk .',\n",
       " 'Good .',\n",
       " 'giving alcohol hard , !',\n",
       " \"Joe , Joe , Joe - n't possible people met reflection hatred ?\",\n",
       " 'Oh please - Bernie us along .',\n",
       " 'Thanks input !',\n",
       " 'I may end haha ... maybe 2000 years drift river found River-Hobbit ring curse becomes disgusting emaciated creature lives cave coughs name verbal tic ...',\n",
       " 'The universe fascinating .',\n",
       " \"It 's cool seeing different ideas spoken ted relate interests .\",\n",
       " \"You check great video technological evolution , 's influence society .\",\n",
       " 'Expand minds , think beautifully ! ! !',\n",
       " 'vimeo.com/44957378',\n",
       " \"It 's Ascension Day .\",\n",
       " 'I celebrating lifting weights .',\n",
       " ': - )',\n",
       " 'I problem sound cutting .',\n",
       " 'Watched subtitles I kept missing picture I reading .',\n",
       " 'I hope fix glitch .',\n",
       " 'Thank .',\n",
       " \"I 'm trying think papercut relation easily could much worse .\",\n",
       " '( Seriously lucky knee injury ! )',\n",
       " 'Appreciate note back Fito often recovery updates soon : ) xo',\n",
       " 'It really inspiring ! ! !',\n",
       " '> > ) ) )',\n",
       " 'Amazing video !',\n",
       " 'I show communication students week inspiration .',\n",
       " 'Thank Monica coming speaking .',\n",
       " 'We needed hear .',\n",
       " 'This must hear talk .',\n",
       " 'Right back ya !',\n",
       " ': )',\n",
       " \"I 'm finding calves get massive DOMs I train directly calf raises .\",\n",
       " 'They pain train comes goes ...',\n",
       " \"I 'd keep bro .\",\n",
       " 'I usually forget DOMs I warm .',\n",
       " 'Man fuck !',\n",
       " \"You 're welcome love .\",\n",
       " 'Thank serving Rep Amash- best best- Iowa !',\n",
       " '# 4Liberty',\n",
       " 'Good work .',\n",
       " 'You close aunt law trailer .',\n",
       " 'I wanted know .',\n",
       " 'Nice know care',\n",
       " 'Perhaps youth getting smarter waking Obama ?',\n",
       " 'Sairal , wan na friends ?',\n",
       " 'We , Daniel Webster Speaker ! !',\n",
       " '!',\n",
       " 'Love love !',\n",
       " 'Just finished watching , I laughed hard I cried .',\n",
       " \"Every time I think I ca n't laugh cry harder , gets greater !\",\n",
       " 'omg This OP shit starter .',\n",
       " 'His previous topics also fabricated .',\n",
       " 'climate control ... truth chemtrails HAARP weather system ... worse volcanoes industry combined ...',\n",
       " 'So happy He * Flemings * dropped sake Party',\n",
       " 'This week , Bill , voted NO reform Fannie Mae & Freddie Mac , received 145 BILLION dollars money .',\n",
       " 'Interestingly , Nancy Pelosi voted NO well .',\n",
       " 'Your voting records indicate lust spending taxpayer money .',\n",
       " 'And wasting well .',\n",
       " 'Why governement even involved ?',\n",
       " \"They 're NOT kids .\",\n",
       " \"She 's going pianist - look fingers !\",\n",
       " 'thanks u !',\n",
       " ': D Nice pic : )',\n",
       " \"`` `` aiding America 's enemies . `` ''\",\n",
       " 'The damage done .',\n",
       " 'The kid paid comparable criminal .',\n",
       " 'What , Liz ?',\n",
       " 'Revenge ?',\n",
       " \"Get Keurig , ..... 'll love !\",\n",
       " 'lol oh thanks Becky ! !',\n",
       " 'Although , I make every day feel like Friday , feel much like Wednesday ? !',\n",
       " 'lol ..',\n",
       " 'Thanks healing stay somewhat sane work ... : )',\n",
       " 'Thanks follow !',\n",
       " 'I teach social studies .',\n",
       " 'Mainly AP Psychology , also Government US History time .',\n",
       " 'Oh yes !',\n",
       " \"But 's well kept guy secret .\",\n",
       " 'awesome ! !',\n",
       " 'I wish MY district ...',\n",
       " 'amazing ! ! ! ! ! !',\n",
       " 'agree ! ! ! !',\n",
       " '!',\n",
       " 'Thanks following back !',\n",
       " 'Hows view high horse ?',\n",
       " 'The description promises excitement Christmas holiday .',\n",
       " \"Let 's spice !\",\n",
       " 'CUT SPENDING , kids grand kids .',\n",
       " 'Great live stream Alicia !',\n",
       " 'shaking head ignorance deliberate ignoring facts FDR , Pearl Harbor , WWII .',\n",
       " 'Let us away , I , roof tall building ... And revel joy sunrise ?',\n",
       " 'Nothing change .',\n",
       " 'It WILL happen .',\n",
       " \"I 'm done Metro .\",\n",
       " \"I fan Jaime , supports Second amendment NRA , I 'm .\",\n",
       " \"Sorry John Kowalski , 're wrong .\",\n",
       " 'thanks !',\n",
       " 'appreciate much .',\n",
       " 'good luck goals .',\n",
       " 'type bike ?',\n",
       " 'Congratulations Congressman Webster ! !',\n",
       " 'We happy !',\n",
       " '!',\n",
       " 'No worries , thanks props !',\n",
       " 'Latrice Smith perfect',\n",
       " 'If go 14:30 realize something funny : Half million people watched film guy , watching film guy , watching film guy , changing oil .',\n",
       " 'I think somehow surreal .',\n",
       " \"indeed , n't understand people would want display/wear another animal 's skin , never .\",\n",
       " 'diversity populations decreasing well ... happier note , little touch end cute , grandparents !',\n",
       " ': )',\n",
       " 'Awww , two favorites !',\n",
       " 'Wish I .',\n",
       " 'Great photo .',\n",
       " 'Thank posting .',\n",
       " \"I sure hope 's movies partnership works .\",\n",
       " 'Dr. Verghese true doctor .',\n",
       " 'He knows takes heal patient .',\n",
       " 'The ritual talks priceless healing patients .',\n",
       " 'His book , `` `` Cutting Stone `` `` , moving talk .',\n",
       " 'He amazing human .',\n",
       " 'Thanks lot : D Will : D !',\n",
       " 'Still .. beautiful creature .',\n",
       " 'I agree I think SGK lose lot supporters , count one !',\n",
       " 'Following fellow IF user .',\n",
       " ': D',\n",
       " 'Hey !',\n",
       " 'Thanks hun !',\n",
       " ': )',\n",
       " 'They know whats year .',\n",
       " 'I doubt mind bring cup germany .',\n",
       " 'Got ta drop say hi every !',\n",
       " ': D',\n",
       " \"What I 'm watching I get home work morning !\",\n",
       " '!',\n",
       " 'heard solar plane South Korea .',\n",
       " 'It seems like incredible machine .',\n",
       " \"coul n't imagine .\",\n",
       " \"wow 's impressive\",\n",
       " 'You loved safe thanks sharing .. =D < 3',\n",
       " 'Thank good example !',\n",
       " 'We need good examples leading country .',\n",
       " 'Maybe women staying home fulfill duty mothers .',\n",
       " 'Perhaps women would less Dads without jobs .',\n",
       " \"I 'm alone neurosis .\",\n",
       " 'Thanks Elizabeth .',\n",
       " 'I heard originally BEZ .',\n",
       " 'Good story !',\n",
       " 'What got Tammy talking helicopter training instructor student killed .',\n",
       " 'It IS dangerous job I much respect pilots soldiers .',\n",
       " 'oooomg sweetest thing world < 333 sending hugs n kisses way xx',\n",
       " \"Let 's !\",\n",
       " 'Amazing courage ingenuity .',\n",
       " 'Thank sir !',\n",
       " 'With year+ hiatus , people I used follow either dropped closed accounts : /Looking find active users : D',\n",
       " 'Of course , thanks original follow',\n",
       " \"Half way , I think , `` `` 's talking samadhi `` `` .. towards end mentions experiencing `` `` nirvana `` `` ... like samadhi .\",\n",
       " 'Samadhi complete quietude mind nirvana means without ego losing sense I , complete union .',\n",
       " 'AWESOME',\n",
       " 'Thanks , Photoshop fun !',\n",
       " 'Forget get rd Obama , rest fix .',\n",
       " 'Awesome Steve !',\n",
       " \"You 'll GREAT job !\",\n",
       " 'Or treat Lyme Disease , causation .',\n",
       " \"Now 's novel idea .\",\n",
       " 'I follow I read bio .',\n",
       " 'Great achievements far good luck triathlons .',\n",
       " 'look forward seeing training : - )',\n",
       " 'Awe .',\n",
       " 'Thanks .',\n",
       " ': ) working change .',\n",
       " 'Thank Jaime working va like l use insurance still charge last year took tax refund witch I told thank Steven Looney',\n",
       " 'Talks exactly I watch TED .',\n",
       " 'Bravo !',\n",
       " 'Slowly surely !',\n",
       " 'Feel better !',\n",
       " 'I love way write .',\n",
       " 'everything .',\n",
       " \"books , stories , blogs , facebook posts , tweets ... 're eloquent drives mad .\",\n",
       " 'good way .',\n",
       " 'thanks inspiration .',\n",
       " 'You got ta figure though teams done soon',\n",
       " 'Muchsimas felicidades Maity , ud es un digno ejemplo de empeo , perseverancia humildad .',\n",
       " 'Dios la siga bendiciendo hoy siempre .',\n",
       " 'Gracias por siempre apoyar lo que viene de nuestro pas por sentirse orgullosa de ser catracha ( transmitir eso travez de la televisin ) .',\n",
       " 'Abrazos !',\n",
       " 'China stable sustainable economy model .',\n",
       " 'Now environment problem becoming severe .',\n",
       " 'The quality economy poor growth slowing .',\n",
       " \"That 's need reform keep growth speed .\",\n",
       " 'Beautiful talented lady/actress !',\n",
       " 'Have heard Charles Einstein ?',\n",
       " 'This guy amazing views world http : //dustantownsend.com/ ? p=659 video evolution civilization consciousness I recommend everyone !',\n",
       " 'like please share .',\n",
       " 'Happy birthday !',\n",
       " 'Sounds like fun day !',\n",
       " 'Very exciting imaginative .',\n",
       " 'I agree would great longer .',\n",
       " 'You kind get used 18 minutes soaking Ted talks .',\n",
       " 'Defiantly would like know see .',\n",
       " 'actually put workout status ya know',\n",
       " 'I Am Xtasie ! !',\n",
       " 'I Am [ [ PHOTO ] ]',\n",
       " \"You 're welcome & Thank following back !\",\n",
       " 'Thank Brene reminding us take care first !',\n",
       " 'Shared nurses Scrubs.com inspired !',\n",
       " 'http : //scrubsmag.com/the-power-of-vulnerability-why-nurses-must-take-care-of-themselves/',\n",
       " 'This third time I watched video - I taking advice using technique works .',\n",
       " 'Wonderful stuff - I also sharing many others .',\n",
       " 'A cynic would say regardless outcome : good , bad , lethal , Trey advocating billable time lawyers , prosecutors , defense , probate .',\n",
       " 'Makes sense think bit .',\n",
       " '( !',\n",
       " ') .',\n",
       " 'My daughter I always look forward seeing Linda stylish self .',\n",
       " \"We 'll miss us .\",\n",
       " 'Enjoy retirement fullest .',\n",
       " 'Looking great guys !',\n",
       " 'All best & travel safe',\n",
       " 'Not watching .',\n",
       " 'Michael Moore ..... really',\n",
       " 'And support killing wildlife trophies .',\n",
       " 'Love Conquered Hate < 3',\n",
       " 'Have great day Amanda !',\n",
       " '!',\n",
       " 'Repeal egregious , bullshit , big government , essence , TAX HIKE .',\n",
       " 'No way ever going see benefit sellout Christie legislation ! !',\n",
       " '!',\n",
       " 'Thank Tyra ... thank caring , , sharing ...',\n",
       " 'Pretty great get ability .',\n",
       " ': ) Also I woke 40 notifications morning',\n",
       " 'This woman never ceases amaze .',\n",
       " 'Thank Molly .',\n",
       " 'Insightful , honest revealing .',\n",
       " 'You definitely found path life .',\n",
       " 'Kudos !',\n",
       " 'Oh I already follow !',\n",
       " 'I want recommend : - ) hehe',\n",
       " 'Rhino Republican Paul Ryan lacky',\n",
       " 'This process safer Monsanto kind genetic engineering .',\n",
       " 'This process happening inside controlled space .',\n",
       " 'So I think kind work good .',\n",
       " 'Mucha suerte pendientes Rubn , saludos !',\n",
       " 'You front effort defund obamacare !',\n",
       " 'I see comments effort !',\n",
       " 'No problemo .',\n",
       " 'Thanks : )',\n",
       " \"Why n't people ignostics ?\",\n",
       " \"Maybe 's personal thing , scientifically-minded person one would think logical choice 're going label .\",\n",
       " '< obligatory > Say goodbye gains . < /obligatory >',\n",
       " 'Patty , weekend plans canvass Yakima Washington Jay Clough YOU ...',\n",
       " 'Go back study constitution .',\n",
       " '5 years accomplished ?',\n",
       " '$ 19+ trillion debt ?',\n",
       " \"You 're one favorite congressmen !\",\n",
       " 'Why go use Comic Sans ?',\n",
       " 'Congratulations good work',\n",
       " 'The DMV particular cousin like exceptions rule .',\n",
       " 'I hate place fiery passion .',\n",
       " 'You must pass legislation Common sense Gun Control .',\n",
       " 'On behalf # EmperorOmariJibri',\n",
       " 'Always good thing hear !',\n",
       " 'Wow , incredible accident .',\n",
       " 'Well , I wish accidents lead great discovery like .',\n",
       " 'Well done girls .',\n",
       " 'You bet ! ! !',\n",
       " 'Happy lifting ! ! !',\n",
       " '!',\n",
       " 'Hi Mary ... glad finally see ...',\n",
       " 'No problem .',\n",
       " 'Nice lift stats !',\n",
       " 'Kick butt !',\n",
       " 'That name sounds familiar ! !',\n",
       " '!',\n",
       " \"remember 're keeping fit life , today .\",\n",
       " 'set short term goals help move towards long term ones .',\n",
       " \"'ve made start 's hardest part\",\n",
       " 'Thank much .',\n",
       " 'Irene I wish Dianne Merry Christmas Happy , Healthy Nww Year .',\n",
       " '( 2/6 ) ng , acting socially , spreading wings I wish another 60 years , progressive science allow .',\n",
       " 'G-d Bless , & I wish meet',\n",
       " 'Like like like .............',\n",
       " 'agree Middle East assessment expressed MTP today ... thank service ...',\n",
       " 'Mr. Gowdy .',\n",
       " 'We love pursuit truth .',\n",
       " 'Please continue pursue Benghazi tragedy well Fast Furious .',\n",
       " 'No flying Legion zones start Legion .',\n",
       " 'Game set broken isles , aka , water .',\n",
       " 'Walking water useful questing traveling .',\n",
       " 'I glad someone stood bullies Of world !',\n",
       " 'You go girl ! !',\n",
       " '!',\n",
       " 'Definitely worth opinion .',\n",
       " 'Many fans calling best work - critics included .',\n",
       " 'So many ads detrimental.They work.I never forget Gingrich Santurum done could ever forget MC Cain South Carolina .',\n",
       " \"I remember 5 days Newt confidently said , I 'm going win , dust\",\n",
       " 'Are playing singles quallies Sydney Vania ?',\n",
       " 'Look forward seeing anyway : )',\n",
       " 'Welcome group , thanks follow back !',\n",
       " 'absolutely beautiful every way',\n",
       " 'Absolutely pretty angel',\n",
       " 'At least beaten death',\n",
       " 'Thanks .',\n",
       " 'Keep great work !',\n",
       " 'ok , lets please something !',\n",
       " '!',\n",
       " 'This talk actually inspired give lecture topic senior citizens region .',\n",
       " 'Awesome !',\n",
       " \"You seem like really positive guy I 'm sure translates great father well .\",\n",
       " 'Lets keep working sons !',\n",
       " 'Thats still got violation * something * .',\n",
       " 'Knowingly something give legal right sue ?',\n",
       " 'Thats completely unethical .',\n",
       " \"Lol I 'm mad cho either `` `` Still away , still away . `` ''\",\n",
       " 'Get `` `` Zen `` `` beach',\n",
       " '27 ( ) NRA leaders held accountable shooting .',\n",
       " 'EVERY NRA MEMBER accomplice tragedy , blood innocent victims THEIR hands !',\n",
       " '( 22 years retired military , combat veteran , WEAPON owner whole life , former republican NOT NRA member ! )',\n",
       " 'Awww !',\n",
       " 'Thank much Mike !',\n",
       " 'Your consistency inspiring .',\n",
       " \"Thank YOU motivation & 's wishing great fitful New Year !\",\n",
       " \"I ca n't seem find group ...\",\n",
       " 'Sorry !',\n",
       " \"If want I avoid propping future 's pain : )\",\n",
       " 'Hey !',\n",
       " 'No problem : ) It pleasure .',\n",
       " \"Ca n't wait see well !\",\n",
       " 'Fantastic talk ... type abstract analysis pushes knowledge , disciples , forward .',\n",
       " 'Also , irrespective topic guy one articulate eloquent speakers I ever heard .',\n",
       " 'I salute sir !',\n",
       " \"Deb , 're worthy reply .\",\n",
       " 'Conversing like talking brick wall .',\n",
       " 'Oops !',\n",
       " 'I apologize insulting brick wall .',\n",
       " 'Keep blocking present opinions opposite .',\n",
       " \"After , regressives like live world ca n't handle truth .\",\n",
       " 'Praying safe return ! !',\n",
       " 'Thank Mr. Williams ! ! !',\n",
       " '!',\n",
       " 'Im sick dog , still volunteering .',\n",
       " \"I 'll get better eventually , nation get better vote Debbie !\",\n",
       " '!',\n",
       " 'Give time I might able steer right direction .',\n",
       " \"I 'll get back later today exercises might like .\",\n",
       " 'While may managed gerrymander perpetual employment , truly represent LUCAS COUNTY .',\n",
       " 'Representative Kaptur champion region ever .',\n",
       " 'Yay !',\n",
       " 'Go Ravenclaws , I say !',\n",
       " \": ) It looks like 're going get hard work , lot fun support .\",\n",
       " 'I see said going !',\n",
       " 'Thank !',\n",
       " 'We rooting Claire !',\n",
       " 'For anyone wants believe humanizing possibilities connected world , anthem .',\n",
       " 'fall , get back : - )',\n",
       " 'lol I worked last night work .',\n",
       " 'Once I got home I chilled family .',\n",
       " \"I n't time put workout yesterday .\",\n",
       " \"I 'll put .\",\n",
       " 'Thanks , support nice : )',\n",
       " \"You 're welcome & Thank follow back ; )\",\n",
       " 'When crafting bill restore election Senators state legislatures ?',\n",
       " 'Whaaaaaat .',\n",
       " 'Youre saying ?',\n",
       " 'Person I responded clear trumpkin',\n",
       " 'It looks nice cozy : )',\n",
       " 'What I say ?',\n",
       " \"I 'm Christian appreciate tight ass !\",\n",
       " 'Thanks follow back : ) loooking forward feed !',\n",
       " 'Thanks following back !',\n",
       " \"Lol , I need come new physics jokes , I think I 've posted every one I know : )\",\n",
       " 'Thank Gio Robin sharing .',\n",
       " 'Am sure CNN good .',\n",
       " 'I feel TV depresses .',\n",
       " 'Esp people shootin color !',\n",
       " 'I hope feeling good .',\n",
       " 'I know MS terrible disease .',\n",
       " 'Hope well .',\n",
       " 'Depends ask .',\n",
       " 'Lets leave General Lee High School Montgomery , AL alone , hows stop adding insult injury , yeah ?',\n",
       " 'Hey Amy beautiful sitting g watching flash point beautiful',\n",
       " 'Nev fucking hot .',\n",
       " 'Lauren & I !',\n",
       " \"aw .... creator 's creature shopping : )\",\n",
       " 'I would gone Spleazeosaur',\n",
       " 'AH !',\n",
       " \"That 's adorable : ) )\",\n",
       " \"Really blows mind conference , great work children 's brilliant statement visual plasticity !\",\n",
       " 'Congrats !',\n",
       " 'No , ... bonuses used pay back tax debts .',\n",
       " 'Didnt suit position ?',\n",
       " 'Muslim friends ?',\n",
       " 'What fuck babbling ?',\n",
       " 'You dont slightest clue youre talking , usual , make blind stupid assumptions .',\n",
       " 'Keep whining .',\n",
       " 'This shit hilarious .',\n",
       " 'thanks , .',\n",
       " 'means lot !',\n",
       " ': )',\n",
       " 'Such raunchy material !',\n",
       " 'I loved The cyber sex scenes , well water breaking scene .',\n",
       " 'Ed awesome !',\n",
       " 'My pleasure course .',\n",
       " 'It felt good',\n",
       " \"Ca n't shake message .\",\n",
       " 'Entertaining engaging simple do-able point .',\n",
       " \"You 're good company\",\n",
       " 'Dina , SPCA really cared animals would give away dogs cats instead killing poor animals .',\n",
       " 'DID fantastic week !',\n",
       " 'Well done .',\n",
       " \"I hope n't mind jumping following bandwagon !\",\n",
       " \"Someone I follow count part fitness support net said 're made win I 'm going follow !\",\n",
       " ': )',\n",
       " 'Haha get track work outs save workouts want try different quests challenges gain different achievements .',\n",
       " 'Each workout submit , ear points points go next level : )',\n",
       " 'Well deserved , honor SD watch .',\n",
       " \"Too bad n't get see US team take home Fed cup !\",\n",
       " \"I almost said personal trainer whenever I squats lunges , 've paying attention clients .\",\n",
       " 'pleasure mine : ) welcome fitocracy !',\n",
       " 'In case !',\n",
       " 'Haha ...',\n",
       " 'Thanks following back .',\n",
       " 'Love PP !',\n",
       " 'When turkey going ready Congresswoman ?',\n",
       " 'Thank services standing election wasnegative slogans make American greatandbuildawalliskkksaying',\n",
       " 'Massage probably work better',\n",
       " 'Pulling young lady !',\n",
       " \"I 'm CoCo fan !\",\n",
       " 'I think world inequality poverty selfishness .',\n",
       " \"It n't matter equal equal mean everybody poor .\",\n",
       " 'Always inequality might something eliminate poverty .',\n",
       " 'To need compassionates .',\n",
       " 'We grateful cheap T-shirts ?',\n",
       " 'Really ?',\n",
       " 'The citizens rest Sacramento pay price City developer caused .',\n",
       " 'Outrage understatement .',\n",
       " 'Awesome ...',\n",
       " 'Thanks Billie Jean .',\n",
       " 'Another rain day race , runners seemed enjoy .',\n",
       " 'Congratulations runners walkers .',\n",
       " 'Race return home dry .',\n",
       " 'Another wonderful day OKC .',\n",
       " 'You two working hard .... thanks stepping - great Congressman .',\n",
       " 'Not confused Randy Quaid ...',\n",
       " 'Thats Im hoping , holding breath .',\n",
       " 'Lets realistic .',\n",
       " 'People really living fear , deathly afraid disappointed .',\n",
       " 'Out fear develop beliefs cause hesitate , give -- -consequently get limited results .',\n",
       " 'Put love behind everything life magical !',\n",
       " 'Good !',\n",
       " \"Let 's say I work , I 'm posting pic ... , I commend !\",\n",
       " \": ) Btw , 's driving crazy ; old username ?\",\n",
       " 'Your boyfriend laugh finds sucking daddy Obama',\n",
       " 'humanity fail double standards .',\n",
       " 'think Iraq , Palestine , Afghanistan ,',\n",
       " 'mcgovern name follow : )',\n",
       " \"We 'll sending SC good vibes way !\",\n",
       " 'Good Congratulations',\n",
       " 'Welp , would work unless earlier group done raid pug , habe bulba , bulba , mon .',\n",
       " 'Your hypothesis flawed .',\n",
       " 'Shelby superb location nice',\n",
       " 'I love writing , Dean ! !',\n",
       " '!',\n",
       " 'Well done .',\n",
       " 'Brilliantly spoken , I hope cyber bullies realise much insidious comments destroy lives .',\n",
       " 'You respect Monica',\n",
       " 'It great know someone Washington changed core values political purposes !',\n",
       " 'Go Mike !',\n",
       " 'Wowee ! !',\n",
       " 'Wonderful voice , Jensen ! !',\n",
       " '< 3',\n",
       " 'Yes Adam position Syria',\n",
       " 'He played fair share ILB us past season .',\n",
       " 'He always used like hybrid player .',\n",
       " 'Boy !',\n",
       " 'Bjorn really let go !',\n",
       " 'This sounds interesting !',\n",
       " \"I wish thirty minutes ' notice ; I guess I 'll wait 're local .\",\n",
       " 'When plan visit ?',\n",
       " 'I think let staff tackle middle office , carry nearest fountain ( pool work ) , lead everyone yell practice .',\n",
       " 'That would school spirit .',\n",
       " \"..... absolutely need governor 's support effort well many environmental challenges facing Maine nation world .\",\n",
       " 'Thank , Chellie , important post .',\n",
       " 'Ultimately , environmental consequences accumulate impact us .',\n",
       " 'Fun fact , A-side composition !',\n",
       " 'source : < link >',\n",
       " 'I nothing admiration & Husband & done make world better place ...',\n",
       " 'ginger beer rum ?',\n",
       " 'taste ?',\n",
       " 'Im intrigued .',\n",
       " 'Happy Birthday ! ! !',\n",
       " 'You living life well ! ! !',\n",
       " 'Best wishes many great years !',\n",
       " 'Mind sport already existing thing though .',\n",
       " 'eSports I would like actual word , Im Oxford .',\n",
       " 'I meant definition sport change include non athletic competition , people would sour even though word athletics .',\n",
       " 'feeling might statistic-ing wrong',\n",
       " 'Gotcha , thanks reply man .',\n",
       " 'Seems like 7th grade generation taught become adult .',\n",
       " 'He resort al caps Twitter .',\n",
       " ': )',\n",
       " 'Piss poor liberal policy caused chicago problems .',\n",
       " 'You blame anything want know truth .',\n",
       " '.',\n",
       " 'AMAZING presentation ! !',\n",
       " 'AMAZING work ! !',\n",
       " 'AMAZING world ! !',\n",
       " 'Magical Breathtaking !',\n",
       " '!',\n",
       " 'Inspirational opening , ladies !',\n",
       " \"Love 'Wabash Red , ' BJK !\",\n",
       " 'Thanks keeping us loop , sir .',\n",
       " 'It funny outrage .',\n",
       " 'I written MP continue concerns heard respected .',\n",
       " 'You know dogs experience life top like people , got ta love !',\n",
       " 'Robin right cost tax payers special election',\n",
       " 'Amazing creature ... replicate large area little time .',\n",
       " 'It must able process information insanely quick ! ...',\n",
       " 'Remarkable sensery perception .',\n",
       " 'thank voting hunting National Parks ... It would scary going knowing hunters shooting animals.It would horrible .... thank patricia macher',\n",
       " 'best news iv heard long time ! ! !',\n",
       " 'congrats !',\n",
       " '!',\n",
       " 'Awesome ! !',\n",
       " \"I 've busy in-laws town I need binge watch last couple episodes .\",\n",
       " 'Great news ..... surprising ! !',\n",
       " \"We know 'winners ' see 'em !\",\n",
       " '< smile >',\n",
       " 'Another corrupt politician HRC shill vote',\n",
       " 'One best TED talks ever .',\n",
       " 'True , inspiring , cool !',\n",
       " 'wonderful fb game.wow.88 https : //www.facebook.com/pages/allflogk/239888956148041 ? sk=app_208195102528120',\n",
       " 'Voting yes Auditing FED best thing could ... Next time , I hope .',\n",
       " 'You fabulous job .',\n",
       " 'You made politics .',\n",
       " 'There NO video : ( ( sound ... : (',\n",
       " 'Chris ALWAYS # 1 hearts .',\n",
       " 'Soror Fudge , victory !',\n",
       " 'Hear , hear !',\n",
       " 'Great , inspiring talk .',\n",
       " 'I going upstander !',\n",
       " 'Go Monica !',\n",
       " 'We .',\n",
       " 'Please fight republicans want hold federal assistance hostage budget cuts',\n",
       " 'Shut take money !',\n",
       " 'Such lovely young ladies .... nice well !',\n",
       " 'Thanks girls !',\n",
       " 'Need dad U.S. Senate !',\n",
       " \"I bet 'd appreciate .\",\n",
       " 'More people heard parallel squat I ever knew existed .',\n",
       " 'That ellipsis ids substitute punctuation ?',\n",
       " 'really awesome , motivating ... im PTing even MORE : P',\n",
       " 'The shock senselessness killings movements mind numbing .',\n",
       " 'That front stop forthcoming also mind numbing .',\n",
       " 'I impressed wit Genie today .',\n",
       " 'She definitely win Grand Slam .',\n",
       " 'Sooner rather later I think .',\n",
       " 'You seemed like away little bit I thought bueno , I prop : )',\n",
       " 'Data available , make something .',\n",
       " 'Oh I hated end pregnancy .',\n",
       " 'It hard !',\n",
       " 'Good luck !',\n",
       " 'I think nesting could considered moderate aerobic/cardio activity : )',\n",
       " '@ Cathy , else would call someone launches wars congressional approval ... demand legislation secretly arrest american citezens ?',\n",
       " 'Or executive order take resources case ANY emergency .',\n",
       " 'Get well soon , giiirrl !',\n",
       " 'Sounds lovely !',\n",
       " 'Have kickass workout relaxing evening !',\n",
       " ': )',\n",
       " 'I sent boss .',\n",
       " 'We try sooooo hard control media focus making actually produce great content .',\n",
       " \"It 's brave new world management ... n't fear truth .\",\n",
       " 'Agree .',\n",
       " \"I 've learned listen body .\",\n",
       " \"It 's smarter brain days .\",\n",
       " ': )',\n",
       " 'An amazing talk , awesome footage .',\n",
       " 'I especially liked two speakers film : young girl made feel humble , old man inspired take step back appreciate luck .',\n",
       " 'Omg lynn jenkins .',\n",
       " ...]"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deleteStopWords(train_mots_path)\n",
    "deleteStopWords(test_mots_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 7) Écrivez une fonction preprocess_corpus(corpus) qui prend un corpus brut stocké dans un fichier.csv, effectue les étapes précédentes, puis stocke le résultat de ces différentes opérations dans un fichier corpus _norm.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def preprocess_corpus(input_file: str, output_file: str) :\n",
    "    #to do : vérifier si c'est bien le résultat voulu\n",
    "    results = deleteStopWords(stemmize(lemmatize(tokenize(normalize(segmentSentences(input_file))))))\n",
    "    file = open(output_file, \"w\")\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for element in results :\n",
    "            r = element.replace('\"', '\"\"').replace('\"', '')\n",
    "            f.write('\"' + r + '\"\\n')\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "preprocess_corpus(\n",
    "   os.path.join(data_path, \"train.csv\"), os.path.join(output_path, \"train_norm.csv\")\n",
    ")\n",
    "preprocess_corpus(\n",
    "   os.path.join(data_path, \"test.csv\"), os.path.join(output_path, \"test_norm.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Exploration des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Complétez les fonctions retournant les informations suivantes (une fonction par information, chaque fonction prenant en argument un corpus composé d'une liste de phrases segmentées en tokens(tokenization)) ou une liste de genres et une liste de sentiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [['soso.', 'kim a acheté un mbp13 silver!', 'mourad a acheté un mbp16 spacegrey']]\n",
    "corpus = [['soso', 'a', 'acheté', 'un', 'mbp16', 'silver', '.', 'Je', 'ne', 'suis', 'pas', 'daccord!'],\n",
    "          ['kim', 'a', 'acheté', 'un', 'mpb13', 'silver','.'], \n",
    "          ['mourad', 'a', 'acheté', 'un', 'mbp16', 'spacegrey', '.', 'Quil', 'aime', 'beaucoup']]\n",
    "corpus = [[\"I\", \"do\", \"not\", \"agree_NEG\", \"with_NEG\", \"this_NEG!\", \"I\", \"prefer\", \"the\", \"other\", \"option\"],\n",
    "           [\"I\", \"really\", \"like\", \"that\", \"new\", \"mbp16\", \"Mourad\", \"made\", \"a\", \"good\", \"choice\"],\n",
    "           [\"I\", \"don't\", \"think_NEG\", \"that_NEG.\", \"I\", \"prefer\", \"the\", \"mbp13\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return if the corpus is tokenized or not\n",
    "def isTokenized(corpus) :\n",
    "    for sentences in corpus :\n",
    "        for sentence in corpus :\n",
    "            for l in sentence :\n",
    "                if ' ' in l :\n",
    "                    return 0\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return the corpus as a list of documents, that are not tokenizated, but segmented in sentences\n",
    "def getListOfSentences(corpus):\n",
    "    listOfDocs = []\n",
    "    listOfTokens = []\n",
    "    if isTokenized(corpus) :\n",
    "        for sentences in corpus :\n",
    "            for token in sentences :\n",
    "                listOfTokens.append(token)\n",
    "            s = ' '.join(listOfTokens)\n",
    "            s = nltk.sent_tokenize(s)\n",
    "            listOfDocs.append(s)\n",
    "            listOfTokens = []\n",
    "    return listOfDocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['I do not agree_NEG with_NEG this_NEG!', 'I prefer the other option'],\n",
       " ['I really like that new mbp16 Mourad made a good choice'],\n",
       " [\"I don't think_NEG that_NEG.\", 'I prefer the mbp13']]"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getListOfSentences(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### a. Le nombre total de tokens (mots non distincts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getNumberOfTokens(corpus):\n",
    "    corpus = getListOfSentences(corpus)\n",
    "    count = 0\n",
    "    for sentences in corpus :\n",
    "        for sentence in sentences :\n",
    "            count = count + len(nltk.word_tokenize(sentence))\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getNumberOfTokens(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### b. Le nombre total de types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getNumberOfTypes(corpus):\n",
    "    corpus = getListOfSentences(corpus)\n",
    "    listOfTokens = []\n",
    "    for sentences in corpus :\n",
    "        for sentence in sentences :\n",
    "            tokenList = nltk.word_tokenize(sentence)\n",
    "            for token in tokenList :\n",
    "                listOfTokens.append(token)\n",
    "    listOfTypes = list(dict.fromkeys(listOfTokens))  \n",
    "    return len(listOfTypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getNumberOfTypes(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### c. Le nombre total de phrases avec négation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getNumberOfNeg(corpus) :\n",
    "    corpus = getListOfSentences(corpus)\n",
    "    numberOfNegativeSentences = 0;\n",
    "    for sentences in corpus:\n",
    "        for sentence in sentences:\n",
    "            if \"_NEG\" in sentence:\n",
    "                numberOfNegativeSentences = numberOfNegativeSentences + 1\n",
    "    return numberOfNegativeSentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getNumberOfNeg(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### d. Le ratio token/type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getRatioTokenType(corpus):\n",
    "    return float(getNumberOfTokens(corpus)/getNumberOfTypes(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2692307692307692"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getRatioTokenType(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### e. Le nombre total de lemmes distincts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "def getLemmesNumber(corpus):\n",
    "    corpus = getListOfSentences(corpus)\n",
    "    lemmzer = nltk.WordNetLemmatizer()\n",
    "    lemmesList = []\n",
    "    for sentences in corpus :\n",
    "        for sentence in sentences :\n",
    "            lemmes = [lemmzer.lemmatize(token) for token in sentence.split()]\n",
    "            for lemme in lemmes :  \n",
    "                lemmesList.append(lemme)\n",
    "    \n",
    "    lemmesList = list(dict.fromkeys(lemmesList))   \n",
    "    return len(lemmesList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getLemmesNumber(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### f. Le nombre total de racines (stems) distinctes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "def getStemsNumber(corpus):\n",
    "    corpus = getListOfSentences(corpus)\n",
    "    stemmer = nltk.PorterStemmer()\n",
    "    stemsList = []\n",
    "    for sentences in corpus :\n",
    "        for sentence in sentences :\n",
    "            stems = [stemmer.stem(token) for token in sentence.split()]\n",
    "            for stem in stems :\n",
    "                stemsList.append(stem)\n",
    "    stemsList = list(dict.fromkeys(stemsList))     \n",
    "    return len(stemsList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getStemsNumber(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### g. Le nombre total de documents (par classe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getNumberOfDocPerClass(sentiments):\n",
    "    countPositive = 0\n",
    "    countNegative = 0\n",
    "    for sentiment in sentiments : \n",
    "        if sentiment : #positif\n",
    "            countPositive = countPositive + 1\n",
    "        else : #negatif\n",
    "            countNegative = countNegative + 1\n",
    "    return countPositive, countNegative\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 5)"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semtiments = [0,\n",
    "          0,\n",
    "          0,\n",
    "          0,\n",
    "          0, #5 negatif\n",
    "          1,\n",
    "          1,\n",
    "          1,\n",
    "          1,\n",
    "          1,\n",
    "          1] #6 positif\n",
    "\n",
    "getNumberOfDocPerClass(semtiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### h. Le nombre total de phrases (par classe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getNumberOfSentencesPerClass(corpus, sentiments) :\n",
    "    corpus = getListOfSentences(corpus)\n",
    "    countSentencesPositives = 0\n",
    "    countSentencesNegatives = 0\n",
    "    for i in range(len(corpus)):\n",
    "        if sentiments[i] : #positive\n",
    "            countSentencesPositives = countSentencesPositives + len(corpus[i])\n",
    "        else : #negative\n",
    "            countSentencesNegatives = countSentencesNegatives + len(corpus[i])          \n",
    "    return countSentencesPositives, countSentencesNegatives   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2)"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiments = [0,1,1]\n",
    "getNumberOfSentencesPerClass(corpus, sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### i. Le nombre total de phrases avec négation (par classe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getNumberOfNegativeSentences(corpus, sentiments) :\n",
    "    corpus = getListOfSentences(corpus)\n",
    "    countNegativeSentencesPositives = 0\n",
    "    countNegativeSentencesNegatives = 0\n",
    "    i = 0;\n",
    "    for sentences in corpus:\n",
    "        for sentence in sentences:\n",
    "            if sentiments[i]: #positive\n",
    "                if \"_NEG\" in sentence:\n",
    "                    countNegativeSentencesPositives = countNegativeSentencesPositives + 1\n",
    "            else: #negative\n",
    "                if \"_NEG\" in sentence:\n",
    "                    countNegativeSentencesNegatives = countNegativeSentencesNegatives + 1\n",
    "            i = i + 1\n",
    "    return countNegativeSentencesPositives, countNegativeSentencesNegatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiments = [0,1,1,1,1]\n",
    "getNumberOfNegativeSentences(corpus, sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### j. Le pourcentage de réponses positives par genre de la personne à qui cette réponse est faite (op_gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "genders = [['M', 'M', 'W', 'W', 'M', 'M', 'W', 'W', 'M', 'M', 'W', 'W']]\n",
    "sentiments = [1 , 0, 1, 0, 1 , 0, 1,\n",
    "             0, 0, 1,  0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getPourcentageOfPositiveReponsesPerGender(genders, sentiments):\n",
    "    countPosM = countPosW = 0\n",
    "    iterator = 0\n",
    "    totalResponse = len(sentiments)\n",
    "  \n",
    "    for sentiment in sentiments:\n",
    "        if sentiment :\n",
    "            if genders[0][iterator] == 'M':\n",
    "                countPosM = countPosM + 1\n",
    "            elif genders[0][iterator] == 'W':\n",
    "                countPosW = countPosW + 1\n",
    "        iterator = iterator + 1\n",
    "  \n",
    "    pourcentageW = float(countPosW / totalResponse) * 100\n",
    "    pourcentageM = float(countPosM / totalResponse) * 100\n",
    "  \n",
    "    return(pourcentageM,pourcentageW)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25.0, 16.666666666666664)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getPourcentageOfPositiveReponsesPerGender(genders, sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 2) Écrivez la fonction explore(corpus, sentiments, genders) qui calcule et affiche toutes ces informations, précédées d'une légende reprenant l’énoncé de chaque question (a,b, ….j)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def explore(\n",
    "    corpus: List[List[str]], sentiments: List[bool], genders: List[Literal[\"M\", \"W\"]]\n",
    ") -> None:\n",
    "    print(\"Le nombre total de tokens (mots non distincts) : \" + getNumberOfTokens(corpus) + \"\\n\")\n",
    "    print(\"Le nombre total de types : \" + getNumberOfTypes(corpus) + \"\\n\")\n",
    "    print(\"Le nombre total de phrases avec négation : \" + getNumberOfNeg(corpus)  + \"\\n\")\n",
    "    print(\"Le ratio token/type : \" + getRatioTokenType(corpus) + \"\\n\")\n",
    "    print(\"Le nombre total de lemmes distincts : \" + getLemmesNumber(corpus) + \"\\n\")\n",
    "    print(\"Le nombre total de racines (stems) distinctes : \" + getStemsNumber(corpus) + \"\\n\")\n",
    "    print(\"Le nombre total de documents (par classe) : \" + getNumberOfDocPerClass(semtiments) + \"\\n\")\n",
    "    print(\"Le nombre total de phrases (par classe) : \" + getNumberOfSentencesPerClass(corpus, sentiments) + \"\\n\")\n",
    "    print(\"Le nombre total de phrases avec négation (par classe) : \" + getNumberOfNegativeSentences(corpus, sentiments)  + \"\\n\")\n",
    "    print(\"Le pourcentage de réponses positives par genre de la personne à qui cette réponse est faite (op_gender) : \" \n",
    "          + getPourcentageOfPositiveReponsesPerGender(sentiments, genders)  + \"\\n\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 3) Calculer une table de fréquence (lemme, rang (le mot le plus fréquent a le rang 1 etc.) ; fréquence (le nombre de fois où il a été vu dans le corpus).  Seuls les N mots les plus fréquents du vocabulaire (N est un paramètre) doivent être gardés. Vous devez stocker les 1000 premières lignes de cette table dans un fichier nommé table_freq.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateFrequences(corpus) :\n",
    "   \n",
    "    corpus = getListOfSentences(corpus)\n",
    "    lemmzer = nltk.WordNetLemmatizer()\n",
    "    lemmesList = []\n",
    "    sorted_dict = {}\n",
    "   \n",
    "    for sentences in corpus :\n",
    "        for sentence in sentences :\n",
    "            lemmes = [lemmzer.lemmatize(token) for token in sentence.split()]\n",
    "            for lemme in lemmes :\n",
    "                lemmesList.append(lemme)\n",
    "               \n",
    "    for word in lemmesList:\n",
    "        if word not in sorted_dict:\n",
    "            sorted_dict[word] = 0\n",
    "        sorted_dict[word] += 1\n",
    "    words = sorted_dict.items()\n",
    "    sorted_lemme = sorted(words, key= lambda kv: kv[1], reverse=True)\n",
    "    #to do : il manque l'écriture dans un csv ?\n",
    "    return sorted_lemme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 5),\n",
       " ('prefer', 2),\n",
       " ('the', 2),\n",
       " ('do', 1),\n",
       " ('not', 1),\n",
       " ('agree_NEG', 1),\n",
       " ('with_NEG', 1),\n",
       " ('this_NEG!', 1),\n",
       " ('other', 1),\n",
       " ('option', 1),\n",
       " ('really', 1),\n",
       " ('like', 1),\n",
       " ('that', 1),\n",
       " ('new', 1),\n",
       " ('mbp16', 1),\n",
       " ('Mourad', 1),\n",
       " ('made', 1),\n",
       " ('a', 1),\n",
       " ('good', 1),\n",
       " ('choice', 1),\n",
       " (\"don't\", 1),\n",
       " ('think_NEG', 1),\n",
       " ('that_NEG.', 1),\n",
       " ('mbp13', 1)]"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculateFrequences(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 2. Classification automatique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### a) Classification  automatique avec un modèle sac de mots (unigrammes), Naive Bayes et la régression logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "En utilisant la librairie scikitLearn et l’algorithme Multinomial Naive Bayes et Logistic Regression, effectuez la classification des textes avec un modèle sac de mots unigramme pondéré avec TF-IDF.  Vous devez entrainer chaque modèle sur l’ensemble d’entrainement et le construire à partir de votre fichier corpus_train.csv. \n",
    "\n",
    "Construisez et sauvegardez votre modèle sac de mots avec les données d’entrainement en testant les pré-traitements suivants (séparément et en combinaison): tokenisation, lemmatisation, stemming, normalisation des négations, et suppression des mots outils. Vous ne devez garder que la combinaison d’opérations qui vous donne les meilleures performances sur le corpus de test. Indiquez dans un commentaire les pré-traitements qui vous amènent à votre meilleure performance (voir la section 3 – évaluation). Il est possible que la combinaison optimale ne soit pas la même selon que vous utilisiez la régression logistique ou Naive Bayes. On s’attend à avoir deux modèles optimaux, un pour Naive Bayes, et un avec régression logistique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataSet(train_csv, test_csv):\n",
    "    trainPath = os.path.join(output_path, train_csv)\n",
    "    testPath = os.path.join(output_path, test_csv)\n",
    "    \n",
    "    trainData = read_data(trainPath)\n",
    "    testData = read_data(testPath)\n",
    "    \n",
    "    return trainData,testData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output/test_stems.csv'"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deleteStopWords(tokenize(normalize(train_phrases_path)))\n",
    "deleteStopWords(tokenize(normalize(test_phrases_path)))\n",
    "stemmize(os.path.join(output_path, \"train_stopWords.csv\"))\n",
    "stemmize(os.path.join(output_path, \"test_stopWords.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#deleteStopWords(stemmize(lemmatize(tokenize(normalize(segmentSentences(input_file))))))\n",
    "training_data, testing_data = getDataSet(\"train_stems.csv\", \"test_stems.csv\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def naiveBayes(train_data, test_data):\n",
    "    vectorizer = TfidfVectorizer()    \n",
    "    vectors = vectorizer.fit_transform(train_data[0])\n",
    "    clf = MultinomialNB(alpha=0.5)\n",
    "    clf.fit(vectors, train_data[1])\n",
    "    \n",
    "    vectors_test = vectorizer.transform(test_data[0])\n",
    "    y_pred = clf.predict(vectors_test)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.85      0.26      0.40       254\n",
      "        True       0.80      0.98      0.88       751\n",
      "\n",
      "    accuracy                           0.80      1005\n",
      "   macro avg       0.82      0.62      0.64      1005\n",
      "weighted avg       0.81      0.80      0.76      1005\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#initial\n",
    "y_pred_bayes = naiveBayes(train_data, test_data)\n",
    "print(classification_report(test_data[1], y_pred_bayes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Régression Logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def logisticsRegression(train_data, test_data):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectors = vectorizer.fit_transform(train_data[0])\n",
    "    model = LogisticRegression(C=1.0)\n",
    "    model.fit(vectors, train_data[1])\n",
    "    \n",
    "    vectors_test = vectorizer.transform(test_data[0])\n",
    "    y_pred = model.predict(vectors_test)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.80      0.53      0.64       254\n",
      "        True       0.86      0.95      0.90       751\n",
      "\n",
      "    accuracy                           0.85      1005\n",
      "   macro avg       0.83      0.74      0.77      1005\n",
      "weighted avg       0.84      0.85      0.84      1005\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_bayes = logisticsRegression(train_data, test_data)\n",
    "print(classification_report(test_data[1], y_pred_bayes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###  b) Autre représentation pour l’analyse de sentiments et classification automatique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "On vous propose maintenant d’utiliser une nouvelle représentation de chaque document à classifier.\n",
    "Vous devez créer à partir de votre corpus la table suivante :\n",
    "\n",
    "| Vocabulaire | Freq-positive | Freq-négative |\n",
    "|-------------|---------------|---------------|\n",
    "| happy | 10 | 1 |\n",
    "| ... | ... | ... |\n",
    "\n",
    "Où :\n",
    "\n",
    "• Vocabulaire représente tous les types (mots uniques) de votre corpus d’entrainement\n",
    "\n",
    "• Freq-positive : représente la somme des fréquences du mot dans tous les documents de la classe positive\n",
    "\n",
    "• Freq-négative : représente la somme des fréquences du mot dans tous les documents de la classe négative\n",
    "\n",
    "Notez qu’en Python, vous pouvez créer un dictionnaire associant à tout (mot, classe) une fréquence.\n",
    "Ensuite il vous suffit de représenter chaque document par un vecteur à 3 dimensions dont le premier élément représente un biais (initialisé à 1), le deuxième élément représente la somme des fréquences positives (freq-pos) de tous les mots uniques (types) du document et enfin le troisième élément représente la somme des fréquences négative (freq-neg) de tous les mots uniques du document. \n",
    "\n",
    "En utilisant cette représentation ainsi que les pré-traitements suggérés, trouvez le meilleur modèle possible en testant la régression logistique et Naive Bayes. Vous ne devez fournir que le code de votre meilleur modèle dans votre notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def getTableOfFrequencies(corpus) :\n",
    "    dictionnary = {}\n",
    "    listOfTokens = []\n",
    "    data = []\n",
    "\n",
    "    for i in range(len(corpus[0])) :\n",
    "        tokenList = nltk.word_tokenize(corpus[0][i])\n",
    "        for token in tokenList :\n",
    "            listOfTokens.append(token)\n",
    "            key = (token, corpus[1][i])\n",
    "            if key in dictionnary : #if the key already exists in dic, increment frequency\n",
    "                dictionnary[key] = dictionnary[key] + 1\n",
    "            else : #if not, create key and initiate frequency to 1\n",
    "                dictionnary.update({key : 1})\n",
    "        listOfTokens = list(dict.fromkeys(listOfTokens)) #list of distinct tokens\n",
    "\n",
    "    for token in listOfTokens :\n",
    "        freq_pos = 0\n",
    "        freq_neg = 0\n",
    "        if (token, True) not in dictionnary and (token, False) not in dictionnary :\n",
    "            freq_pos = 0\n",
    "            freq_neg = 0\n",
    "        elif (token, True) in dictionnary and (token, False) in dictionnary :\n",
    "            freq_pos = dictionnary[token, True]\n",
    "            freq_neg = dictionnary[token, False]\n",
    "        elif (token, True) in dictionnary and (token, False) not in dictionnary :\n",
    "            freq_pos = dictionnary[token, True]\n",
    "            freq_neg = 0\n",
    "        elif (token, True) not in dictionnary and (token, False) in dictionnary :\n",
    "            freq_neg = dictionnary[token, False]\n",
    "            freq_pos = 0   \n",
    "        data.append([token, freq_pos, freq_neg])    \n",
    "\n",
    "    return pd.DataFrame(data, columns=[\"Vocabulaire\", \"Freq-positive\", \"Freq-négative\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vocabulaire</th>\n",
       "      <th>Freq-positive</th>\n",
       "      <th>Freq-négative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>thanks</td>\n",
       "      <td>95</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>back</td>\n",
       "      <td>42</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>!</td>\n",
       "      <td>776</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yep</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>,</td>\n",
       "      <td>355</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3963</th>\n",
       "      <td>msnbc</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3964</th>\n",
       "      <td>tingles</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3965</th>\n",
       "      <td>ship</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3966</th>\n",
       "      <td>fiber</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3967</th>\n",
       "      <td>mathematicians</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3968 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Vocabulaire  Freq-positive  Freq-négative\n",
       "0             thanks             95              2\n",
       "1               back             42              6\n",
       "2                  !            776            105\n",
       "3                yep              1              0\n",
       "4                  ,            355            176\n",
       "...              ...            ...            ...\n",
       "3963           msnbc              1              0\n",
       "3964         tingles              1              0\n",
       "3965            ship              1              0\n",
       "3966           fiber              1              0\n",
       "3967  mathematicians              1              0\n",
       "\n",
       "[3968 rows x 3 columns]"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getTableOfFrequencies(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 3. Évaluation et discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### a) Pour déterminer la performance de vos modèles, vous devez tester vos modèles de classification sur l’ensemble de test et générer vos résultats pour chaque modèle dans une table avec les métriques suivantes : Accuracy et pour chaque classe, la précision, le rappel et le F1 score. On doit voir cette table générée dans votre notebook avec la liste de vos modèles de la section 2 et leurs performances respectives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_pred_bayes = naiveBayes(training_data, testing_data)\n",
    "# print(classification_report(test_data[1], y_pred_bayes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.80      0.53      0.64       254\n",
      "        True       0.86      0.95      0.90       751\n",
      "\n",
      "    accuracy                           0.85      1005\n",
      "   macro avg       0.83      0.74      0.77      1005\n",
      "weighted avg       0.84      0.85      0.84      1005\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_regression = logisticsRegression(train_data, test_data)\n",
    "print(classification_report(test_data[1], y_pred_regression))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### b) Générez un graphique qui représente la performance moyenne (mean accuracy – 10 Fold cross-validation) de vos différents modèles par tranches de 500 textes sur l’ensemble d’entrainement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### c) Que se passe-t-il lorsque le paramètre de régularisation de la régression logisque (C) est augmenté ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyse et discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) En considérant les deux types de représentations, répondez aux question suivantes en reportant la question dans le notebook et en inscrivant votre réponse:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Quel est l’impact de l’annotation de la négation ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) La suppression des stopwords est-elle une bonne idée pour l’analyse de sentiments ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Le stemming et/ou la lemmatisation sont-ils souhaitables dans le cadre de l’analyse de sentiments ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Contribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complétez la section en haut du notebook indiquant la contribution de chaque membre de l’équipe en indiquant ce qui a été effectué par chaque membre et le pourcentage d’effort du membre dans le TP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
