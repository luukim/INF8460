{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XYaltRsowqfU"
   },
   "source": [
    "## √âcole Polytechnique de Montr√©al\n",
    "## D√©partement G√©nie Informatique et G√©nie Logiciel\n",
    "\n",
    "## INF8460 ‚Äì Traitement automatique de la langue naturelle - TP2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "92idNN-XwvMP"
   },
   "source": [
    "## Objectifs d'apprentissage: \n",
    "\n",
    "‚Ä¢\tExplorer les mod√®les d‚Äôespace vectoriel (vector space models) comme repr√©sentations distribu√©es de la s√©mantique des mots \n",
    "‚Ä¢\tImpl√©menter la fr√©quence de co-occurrence et la PPMI\n",
    "‚Ä¢\tComprendre diff√©rentes mesures de distance entre vecteurs de mots \n",
    "‚Ä¢\tExplorer l‚Äôint√©r√™t de la r√©duction de dimensionnalit√© \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wEoOfK-Rw3zT"
   },
   "source": [
    "## √âquipe et contributions \n",
    "Veuillez indiquer la contribution effective de chaque membre de l'√©quipe en pourcentage et en indiquant les modules ou questions sur lesquelles chaque membre a travaill√©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0mJOdtlw4xC"
   },
   "source": [
    "Luu Thien-Kim: x% (d√©tail)\n",
    "\n",
    "Nom √âtudiant 2: x% (d√©tail)\n",
    "\n",
    "Nom √âtudiant 3: x% (d√©tail)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qu3pnt6mPiNZ"
   },
   "source": [
    "\n",
    "\n",
    "## Support de google Colab\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "id": "x-U9ANA6ZV27",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "# !wget https://staff.fnwi.uva.nl/e.bruni/resources/MEN.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "id": "mVrz-1_uZApz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! tar -xzf aclImdb_v1.tar.gz\n",
    "# ! tar -xzf MEN.tar.gz\n",
    "# ! mkdir -p vsm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "id": "rK102dx5Poti",
    "outputId": "35afe599-60b2-4bfd-d5da-debb0d73bb5c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mouradyounes/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/mouradyounes/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TewW7eDjPbjs"
   },
   "source": [
    "## Librairies externes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "id": "kClu3RY7YFfv"
   },
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from itertools import chain\n",
    "import csv\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords as all_stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.spatial.distance import euclidean, cosine\n",
    "from IPython.display import display\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7oRJolmeRlxW"
   },
   "source": [
    "## Valeurs globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "id": "QqfHWnRoRlFY"
   },
   "outputs": [],
   "source": [
    "DIRNAME_ACL =  os.path.join(os.getcwd(), \"aclImdb\")\n",
    "DIRNAME_MEN =  os.path.join(os.getcwd(), \"MEN\")\n",
    "DIRNAME_VSM =  os.path.join(os.getcwd(), \"vsm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "hGQ15KsihAOp"
   },
   "source": [
    "## 1. Pr√©traitement (20 points)\n",
    "\n",
    "**a)**\tLe jeu de donn√©es est s√©par√© en deux r√©pertoires `train/`et `test`, chacun contenant eux-m√™mes deux sous-r√©pertoires `pos/` et `neg/` pour les revues positives et n√©gatives. Un fichier `readme` d√©crit plus pr√©cis√©ment les donn√©es. Commencez par lire ces donn√©es, en gardant s√©par√©es les donn√©es d'entra√Ænement et de test. La fonction doit mettre les mots en minuscules,  supprimer les stopwords (vous devez utiliser ceux de NLTK) et afficher le nombre total de phrases d‚Äôentrainement,  le nombre total de phrases d‚Äôentrainement positives et n√©gatives et le nombre total de phrases de test avec le nombre total de phrases de test positives et n√©gatives ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "id": "U25ZTtBGO39i"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "all_stopwords.words(\"english\")\n",
    "stopwords_english = set(all_stopwords.words(\"english\"))\n",
    "\n",
    "TRAIN_POS_DIRECTORY = DIRNAME_ACL + '/train/pos'\n",
    "TRAIN_NEG_DIRECTORY = DIRNAME_ACL + '/train/neg'\n",
    "TEST_POS_DIRECTORY = DIRNAME_ACL + '/test/pos'\n",
    "TEST_NEG_DIRECTORY = DIRNAME_ACL + '/test/neg'\n",
    "\n",
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\") #enl√®ve toutes les ponctuations des phrases\n",
    "\n",
    "#List[List[]] telle que [[tokens du texte 1], [tokens du texte 2], ...]\n",
    "trainingSet = [] \n",
    "testingSet = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "hidden": true,
    "id": "5Lk9pYtUKWvf"
   },
   "outputs": [],
   "source": [
    "def getInfo() :\n",
    "    train_pos_files = os.listdir(TRAIN_POS_DIRECTORY)\n",
    "    train_neg_files = os.listdir(TRAIN_NEG_DIRECTORY)\n",
    "    test_pos_files = os.listdir(TEST_POS_DIRECTORY)\n",
    "    test_neg_files = os.listdir(TEST_NEG_DIRECTORY)\n",
    "    \n",
    "    files = [train_pos_files, train_neg_files, test_pos_files, test_neg_files]\n",
    "    \n",
    "    training_sentences_nb = 0\n",
    "    training_sentences_pos_nb = 0\n",
    "    training_sentences_neg_nb = 0\n",
    "\n",
    "    testing_sentences_nb = 0\n",
    "    testing_sentences_pos_nb = 0\n",
    "    testing_sentences_neg_nb = 0\n",
    "    \n",
    "    for listOfFiles in files :\n",
    "        directory = \"\"\n",
    "        if listOfFiles == train_pos_files :\n",
    "            directory = TRAIN_POS_DIRECTORY\n",
    "        elif listOfFiles == train_neg_files :\n",
    "            directory = TRAIN_NEG_DIRECTORY\n",
    "        elif listOfFiles == test_pos_files :\n",
    "            directory = TEST_POS_DIRECTORY\n",
    "        elif listOfFiles == test_neg_files :\n",
    "            directory = TEST_NEG_DIRECTORY\n",
    "            \n",
    "        for file in listOfFiles:\n",
    "            with open(directory + '/' + file, \"r\") as f:\n",
    "                data = list(f)[0].lower()       \n",
    "                tokens = [token for token in tokenizer.tokenize(data) if token not in stopwords_english] #enl√®ve toutes les ponctuations des phrases\n",
    "                data = [token for token in nltk.word_tokenize(data) if token not in stopwords_english]\n",
    "                data = ' '.join(data)\n",
    "                \n",
    "                if \"train\" in directory :\n",
    "                    trainingSet.append(tokens)\n",
    "                    if \"pos\" in directory : \n",
    "                        training_sentences_pos_nb += len(nltk.sent_tokenize(data))\n",
    "                    elif \"neg\" in directory :\n",
    "                        training_sentences_neg_nb += len(nltk.sent_tokenize(data))\n",
    "                    \n",
    "                elif \"test\" in directory :\n",
    "                    testingSet.append(tokens)\n",
    "                    if \"pos\" in directory : \n",
    "                        testing_sentences_pos_nb += len(nltk.sent_tokenize(data))\n",
    "                    elif \"neg\" in directory :\n",
    "                        testing_sentences_neg_nb += len(nltk.sent_tokenize(data))\n",
    "            \n",
    "    training_sentences_nb = training_sentences_pos_nb + training_sentences_neg_nb\n",
    "    testing_sentences_nb = testing_sentences_pos_nb + testing_sentences_neg_nb\n",
    "    \n",
    "    print(\"nombre total de phrases d‚Äôentrainement : \", training_sentences_nb)\n",
    "    print(\"nombre total de phrases d‚Äôentrainement positives : \", training_sentences_pos_nb)\n",
    "    print(\"nombre total de phrases d‚Äôentrainement n√©gatives : \", training_sentences_neg_nb)        \n",
    "    \n",
    "    print(\"nombre total de phrases de test : \", testing_sentences_nb)\n",
    "    print(\"nombre total de phrases de test positives : \", testing_sentences_pos_nb)\n",
    "    print(\"nombre total de phrases de test n√©gatives : \", testing_sentences_neg_nb)     \n",
    "    \n",
    "#     return training_sentences_nb, training_sentences_pos_nb, training_sentences_neg_nb, testing_sentences_nb, testing_sentences_pos_nb, testing_sentences_neg_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "id": "QyrcQloEO8Hj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre total de phrases d‚Äôentrainement :  316801\n",
      "nombre total de phrases d‚Äôentrainement positives :  153934\n",
      "nombre total de phrases d‚Äôentrainement n√©gatives :  162867\n",
      "nombre total de phrases de test :  310594\n",
      "nombre total de phrases de test positives :  150268\n",
      "nombre total de phrases de test n√©gatives :  160326\n"
     ]
    }
   ],
   "source": [
    "getInfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "_tVXpVcSh9eY"
   },
   "source": [
    "**a)**\tCr√©ez la fonction `build_voc()` qui extrait les unigrammes de l‚Äôensemble d‚Äôentra√Ænement et conserve ceux qui ont une fr√©quence d‚Äôoccurrence d'au moins 5 et imprime le nombre de mots dans le vocabulaire. Sauvegardez-le dans un fichier `vocab.txt` (un mot par ligne) dans le r√©pertoire aclImdb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "hidden": true,
    "id": "9IVMVY3Qhx3L"
   },
   "outputs": [],
   "source": [
    "def build_voc(corpus, unk_cutoff=5):\n",
    "    count = 0\n",
    "    dict_ = {}\n",
    "    \n",
    "    for tokens in trainingSet :\n",
    "        for token in tokens :\n",
    "            if token not in dict_ :\n",
    "                dict_[token] = 0\n",
    "            dict_[token] += 1\n",
    "            \n",
    "    newFilePath = DIRNAME_ACL + '/vocab.txt'\n",
    "    with open(newFilePath, \"w\") as f: \n",
    "        for key in dict_ :\n",
    "            if dict_[key] >= unk_cutoff :\n",
    "                count += 1\n",
    "                f.write(key + \"\\n\")\n",
    "                \n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "NDR95Q5PaBGQ",
    "outputId": "d12400de-2d7d-41ac-fa99-7a28f9f660e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28962"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_voc(trainingSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "oEIUW6EpnvQV"
   },
   "source": [
    "## 2. Matrices de co-occurence (30 points)\n",
    "\n",
    "Pour les matrices de cette section, vous pourrez utiliser [des array `numpy`](https://docs.scipy.org/doc/numpy/reference/arrays.ndarray.html) ou des DataFrame [`pandas`](https://pandas.pydata.org/pandas-docs/stable/). \n",
    "\n",
    "Ressources utiles :  le [*quickstart tutorial*](https://numpy.org/devdocs/user/quickstart.html) de numpy et le guide [10 minutes to pandas](https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "fadNCxBcnyeT"
   },
   "source": [
    "**a)** A partir des textes du corpus d‚Äôentrainement (neg/pos), vous devez construire une matrice de co-occurrence mot √ó mot M(w,w) qui contient les 5000 unigrammes les plus fr√©quents sous forme de **cadre panda**. Le contexte de co-occurrence est une fen√™tre de +/-5 mots autour du mot cible. Le poids est la fr√©quence de co-occurrence simple. Sauvegardez votre matrice dans un fichier tp2_mat5.csv dans le r√©pertoire vsm.\n",
    "\n",
    "Attention, le mot lui m√™me de doit pas √™tre compt√© dans sa co-occurence. Exemple : \n",
    "Corpus: [ \"I go to school every day by bus\", \"i go to theatre every night by bus\"]\n",
    "\n",
    "Co-occurence(\"every\", fenetre=2) = [ (to, 2), (by, 2), (school, 1), (day, 1), (theatre, 1), (night, 1), (bus, 0), (every, 0), (go, 0). (i,0) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "hidden": true,
    "id": "Y-diwyiPno2k"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "corpus = [ \"I go to school every day by bus\", \"i go to theatre every night by bus\"]\n",
    "corpus1 = [[\"I\",\"go\",\"to\",\"school\",\"every\",\"day\",\"by\",\"bus\"], [\"i\",\"go\",\"to\",\"theatre\",\"every\",\"night\",\"by\",\"bus\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102.74273800849915\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>gets</th>\n",
       "      <th>respect</th>\n",
       "      <th>sure</th>\n",
       "      <th>lot</th>\n",
       "      <th>memorable</th>\n",
       "      <th>quotes</th>\n",
       "      <th>listed</th>\n",
       "      <th>gem</th>\n",
       "      <th>imagine</th>\n",
       "      <th>...</th>\n",
       "      <th>macarthur</th>\n",
       "      <th>uwe</th>\n",
       "      <th>boll</th>\n",
       "      <th>seagal</th>\n",
       "      <th>porno</th>\n",
       "      <th>zizek</th>\n",
       "      <th>rambo</th>\n",
       "      <th>damme</th>\n",
       "      <th>prom</th>\n",
       "      <th>drivel</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>movie</th>\n",
       "      <td>0.0</td>\n",
       "      <td>353.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>493.0</td>\n",
       "      <td>763.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gets</th>\n",
       "      <td>353.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>respect</th>\n",
       "      <td>71.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sure</th>\n",
       "      <td>493.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lot</th>\n",
       "      <td>763.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zizek</th>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rambo</th>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>damme</th>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prom</th>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>drivel</th>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows √ó 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         movie   gets  respect   sure    lot  memorable  quotes  listed   gem  \\\n",
       "0                                                                               \n",
       "movie      0.0  353.0     71.0  493.0  763.0       80.0    14.0    24.0  64.0   \n",
       "gets     353.0    0.0      7.0   23.0   46.0        7.0     1.0     1.0   0.0   \n",
       "respect   71.0    7.0      0.0    2.0   15.0        2.0     1.0     1.0   0.0   \n",
       "sure     493.0   23.0      2.0    0.0   65.0        7.0     3.0     4.0   5.0   \n",
       "lot      763.0   46.0     15.0   65.0    0.0        4.0     3.0     2.0   2.0   \n",
       "...        ...    ...      ...    ...    ...        ...     ...     ...   ...   \n",
       "zizek      7.0    1.0      0.0    1.0    2.0        0.0     0.0     0.0   0.0   \n",
       "rambo     10.0    3.0      0.0    2.0    1.0        0.0     0.0     0.0   0.0   \n",
       "damme      9.0    3.0      0.0    0.0    0.0        0.0     0.0     0.0   0.0   \n",
       "prom      10.0    2.0      0.0    0.0    1.0        1.0     0.0     0.0   0.0   \n",
       "drivel    13.0    1.0      1.0    1.0    1.0        0.0     0.0     0.0   0.0   \n",
       "\n",
       "         imagine  ...  macarthur   uwe  boll  seagal  porno  zizek  rambo  \\\n",
       "0                 ...                                                       \n",
       "movie      117.0  ...        4.0  19.0  22.0    26.0   23.0    7.0   10.0   \n",
       "gets         7.0  ...        1.0   0.0   1.0     1.0    2.0    1.0    3.0   \n",
       "respect      1.0  ...        0.0   0.0   0.0     1.0    0.0    0.0    0.0   \n",
       "sure         9.0  ...        0.0   0.0   1.0     2.0    1.0    1.0    2.0   \n",
       "lot         14.0  ...        0.0   2.0   1.0     3.0    0.0    2.0    1.0   \n",
       "...          ...  ...        ...   ...   ...     ...    ...    ...    ...   \n",
       "zizek        0.0  ...        0.0   0.0   0.0     0.0    0.0    0.0    0.0   \n",
       "rambo        0.0  ...        0.0   0.0   0.0     0.0    0.0    0.0    0.0   \n",
       "damme        0.0  ...        0.0   0.0   0.0     3.0    0.0    0.0    0.0   \n",
       "prom         0.0  ...        0.0   0.0   0.0     0.0    0.0    0.0    0.0   \n",
       "drivel       0.0  ...        0.0   0.0   0.0     0.0    0.0    0.0    0.0   \n",
       "\n",
       "         damme  prom  drivel  \n",
       "0                             \n",
       "movie      9.0  10.0    13.0  \n",
       "gets       3.0   2.0     1.0  \n",
       "respect    0.0   0.0     1.0  \n",
       "sure       0.0   0.0     1.0  \n",
       "lot        0.0   1.0     1.0  \n",
       "...        ...   ...     ...  \n",
       "zizek      0.0   0.0     0.0  \n",
       "rambo      0.0   0.0     0.0  \n",
       "damme      0.0   0.0     0.0  \n",
       "prom       0.0   0.0     0.0  \n",
       "drivel     0.0   0.0     0.0  \n",
       "\n",
       "[5000 rows x 5000 columns]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def buildMatrix(trainingSet) :\n",
    "    dict_ = {}\n",
    "    words = []\n",
    "\n",
    "    for tokens in trainingSet :\n",
    "        for token in tokens :\n",
    "            if token not in dict_ :\n",
    "                dict_[token] = {}\n",
    "            words.append(token)\n",
    "\n",
    "        for i in range(len(tokens)) :\n",
    "            frame = [i-5, i-4, i-3, i-2, i-1, i+1, i+2, i+3, i+4, i+5]\n",
    "            temp = frame.copy()\n",
    "            for j in range(len(frame)) :\n",
    "                if frame[j] < 0 or frame[j] > len(tokens)-1 :\n",
    "                    temp.remove(frame[j])\n",
    "            frame = temp\n",
    "\n",
    "            neighbouring_tokens_dict = dict_[tokens[i]]\n",
    "\n",
    "            for index in frame :\n",
    "                if tokens[i] == tokens[index]:\n",
    "                    continue\n",
    "                if tokens[index] not in neighbouring_tokens_dict :\n",
    "                    neighbouring_tokens_dict[tokens[index]] = 0\n",
    "                neighbouring_tokens_dict[tokens[index]] += 1\n",
    "\n",
    "    c = Counter(words)\n",
    "    words_to_display = c.most_common(5000)\n",
    "    \n",
    "    w = []\n",
    "    for word in words_to_display :\n",
    "        w.append(word[0])\n",
    "\n",
    "    d = dict_.copy()\n",
    "\n",
    "    for key in dict_ :\n",
    "        if key not in w :\n",
    "            d.pop(key)\n",
    "\n",
    "    vector = [(k, v) for k, v in d.items()]\n",
    "\n",
    "    df1 = pd.DataFrame(vector)\n",
    "    df2 = pd.json_normalize(df1[1])\n",
    "\n",
    "    df2 = df2[df1[0]].fillna(0).set_index(df1[0])\n",
    "    \n",
    "    return df2\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "result = buildMatrix(trainingSet)\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVoc():\n",
    "    voc = []\n",
    "    voc_path = DIRNAME_ACL + '/vocab.txt'\n",
    "    with open(voc_path, \"r\") as f: \n",
    "        data = list(f)\n",
    "        for word in data:\n",
    "            voc.append(word.strip('\\n'))\n",
    "        \n",
    "    return voc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = getVoc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "-vzXMLl3rEBl"
   },
   "source": [
    "**b)** Calculez maintenant une matrice de cooccurrence mais en ajustant les fr√©quences bas√©es sur la proximit√© du mot cible par exemple en les multipliant par 1/ùëë o√π d est la distance en jetons (mots) de la cible. Sauvegardez votre matrice (toujours sous forme de cadre panda) dans un fichier tp2_mat5_scaled.csv dans le r√©pertoire vsm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163.49101901054382\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>gets</th>\n",
       "      <th>respect</th>\n",
       "      <th>sure</th>\n",
       "      <th>lot</th>\n",
       "      <th>memorable</th>\n",
       "      <th>quotes</th>\n",
       "      <th>listed</th>\n",
       "      <th>gem</th>\n",
       "      <th>imagine</th>\n",
       "      <th>...</th>\n",
       "      <th>macarthur</th>\n",
       "      <th>uwe</th>\n",
       "      <th>boll</th>\n",
       "      <th>seagal</th>\n",
       "      <th>porno</th>\n",
       "      <th>zizek</th>\n",
       "      <th>rambo</th>\n",
       "      <th>damme</th>\n",
       "      <th>prom</th>\n",
       "      <th>drivel</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>movie</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>411.843663</td>\n",
       "      <td>82.985696</td>\n",
       "      <td>583.154166</td>\n",
       "      <td>921.418449</td>\n",
       "      <td>94.587893</td>\n",
       "      <td>17.560503</td>\n",
       "      <td>28.607931</td>\n",
       "      <td>82.299414</td>\n",
       "      <td>140.447309</td>\n",
       "      <td>...</td>\n",
       "      <td>5.553607</td>\n",
       "      <td>20.647844</td>\n",
       "      <td>24.636377</td>\n",
       "      <td>31.347149</td>\n",
       "      <td>29.751866</td>\n",
       "      <td>7.191207</td>\n",
       "      <td>10.205420</td>\n",
       "      <td>11.386255</td>\n",
       "      <td>10.954361</td>\n",
       "      <td>18.814542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gets</th>\n",
       "      <td>411.843663</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.666667</td>\n",
       "      <td>30.202893</td>\n",
       "      <td>65.384639</td>\n",
       "      <td>10.050000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.181252</td>\n",
       "      <td>...</td>\n",
       "      <td>1.003115</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.005376</td>\n",
       "      <td>1.007692</td>\n",
       "      <td>2.450000</td>\n",
       "      <td>1.006849</td>\n",
       "      <td>4.012500</td>\n",
       "      <td>3.073098</td>\n",
       "      <td>2.395833</td>\n",
       "      <td>1.006494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>respect</th>\n",
       "      <td>82.985696</td>\n",
       "      <td>11.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.111111</td>\n",
       "      <td>21.094565</td>\n",
       "      <td>2.583333</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.011765</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.006061</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sure</th>\n",
       "      <td>583.154166</td>\n",
       "      <td>30.202893</td>\n",
       "      <td>3.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98.056881</td>\n",
       "      <td>8.903436</td>\n",
       "      <td>4.166667</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>13.202959</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.003086</td>\n",
       "      <td>3.015152</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.019231</td>\n",
       "      <td>2.016846</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lot</th>\n",
       "      <td>921.418449</td>\n",
       "      <td>65.384639</td>\n",
       "      <td>21.094565</td>\n",
       "      <td>98.056881</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.400000</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>2.833333</td>\n",
       "      <td>2.583333</td>\n",
       "      <td>17.379832</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.202660</td>\n",
       "      <td>1.020833</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.022867</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.009434</td>\n",
       "      <td>1.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zizek</th>\n",
       "      <td>7.191207</td>\n",
       "      <td>1.006849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.019231</td>\n",
       "      <td>2.022867</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rambo</th>\n",
       "      <td>10.205420</td>\n",
       "      <td>4.012500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.016846</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>damme</th>\n",
       "      <td>11.386255</td>\n",
       "      <td>3.073098</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prom</th>\n",
       "      <td>10.954361</td>\n",
       "      <td>2.395833</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.009434</td>\n",
       "      <td>1.035714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>drivel</th>\n",
       "      <td>18.814542</td>\n",
       "      <td>1.006494</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows √ó 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              movie        gets    respect        sure         lot  memorable  \\\n",
       "0                                                                               \n",
       "movie      0.000000  411.843663  82.985696  583.154166  921.418449  94.587893   \n",
       "gets     411.843663    0.000000  11.666667   30.202893   65.384639  10.050000   \n",
       "respect   82.985696   11.666667   0.000000    3.111111   21.094565   2.583333   \n",
       "sure     583.154166   30.202893   3.111111    0.000000   98.056881   8.903436   \n",
       "lot      921.418449   65.384639  21.094565   98.056881    0.000000   6.400000   \n",
       "...             ...         ...        ...         ...         ...        ...   \n",
       "zizek      7.191207    1.006849   0.000000    1.019231    2.022867   0.000000   \n",
       "rambo     10.205420    4.012500   0.000000    2.016846    1.333333   0.000000   \n",
       "damme     11.386255    3.073098   0.000000    0.000000    0.000000   0.000000   \n",
       "prom      10.954361    2.395833   0.000000    0.000000    1.009434   1.035714   \n",
       "drivel    18.814542    1.006494   1.250000    1.250000    1.333333   0.000000   \n",
       "\n",
       "            quotes     listed        gem     imagine  ...  macarthur  \\\n",
       "0                                                     ...              \n",
       "movie    17.560503  28.607931  82.299414  140.447309  ...   5.553607   \n",
       "gets      1.200000   1.500000   0.000000    9.181252  ...   1.003115   \n",
       "respect   1.250000   1.200000   0.000000    1.011765  ...   0.000000   \n",
       "sure      4.166667   6.250000   6.900000   13.202959  ...   0.000000   \n",
       "lot       5.500000   2.833333   2.583333   17.379832  ...   0.000000   \n",
       "...            ...        ...        ...         ...  ...        ...   \n",
       "zizek     0.000000   0.000000   0.000000    0.000000  ...   0.000000   \n",
       "rambo     0.000000   0.000000   0.000000    0.000000  ...   0.000000   \n",
       "damme     0.000000   0.000000   0.000000    0.000000  ...   0.000000   \n",
       "prom      0.000000   0.000000   0.000000    0.000000  ...   0.000000   \n",
       "drivel    0.000000   0.000000   0.000000    0.000000  ...   0.000000   \n",
       "\n",
       "               uwe       boll     seagal      porno     zizek      rambo  \\\n",
       "0                                                                          \n",
       "movie    20.647844  24.636377  31.347149  29.751866  7.191207  10.205420   \n",
       "gets      0.000000   1.005376   1.007692   2.450000  1.006849   4.012500   \n",
       "respect   0.000000   0.000000   1.006061   0.000000  0.000000   0.000000   \n",
       "sure      0.000000   1.003086   3.015152   1.333333  1.019231   2.016846   \n",
       "lot       2.202660   1.020833   5.250000   0.000000  2.022867   1.333333   \n",
       "...            ...        ...        ...        ...       ...        ...   \n",
       "zizek     0.000000   0.000000   0.000000   0.000000  0.000000   0.000000   \n",
       "rambo     0.000000   0.000000   0.000000   0.000000  0.000000   0.000000   \n",
       "damme     0.000000   0.000000   4.083333   0.000000  0.000000   0.000000   \n",
       "prom      0.000000   0.000000   0.000000   0.000000  0.000000   0.000000   \n",
       "drivel    0.000000   0.000000   0.000000   0.000000  0.000000   0.000000   \n",
       "\n",
       "             damme       prom     drivel  \n",
       "0                                         \n",
       "movie    11.386255  10.954361  18.814542  \n",
       "gets      3.073098   2.395833   1.006494  \n",
       "respect   0.000000   0.000000   1.250000  \n",
       "sure      0.000000   0.000000   1.250000  \n",
       "lot       0.000000   1.009434   1.333333  \n",
       "...            ...        ...        ...  \n",
       "zizek     0.000000   0.000000   0.000000  \n",
       "rambo     0.000000   0.000000   0.000000  \n",
       "damme     0.000000   0.000000   0.000000  \n",
       "prom      0.000000   0.000000   0.000000  \n",
       "drivel    0.000000   0.000000   0.000000  \n",
       "\n",
       "[5000 rows x 5000 columns]"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_distance_matrix(trainingSet) :\n",
    "    dict_ = {}\n",
    "    words = []\n",
    "\n",
    "    for tokens in trainingSet :\n",
    "        for token in tokens :\n",
    "            if token not in dict_ :\n",
    "                dict_[token] = {}\n",
    "            words.append(token)\n",
    "\n",
    "        for i in range(len(tokens)) :\n",
    "            frame = [i-5, i-4, i-3, i-2, i-1, i+1, i+2, i+3, i+4, i+5]\n",
    "            tempFrame = frame.copy()\n",
    "            for j in range(len(frame)) :\n",
    "                if frame[j] < 0 or frame[j] > len(tokens)-1 :\n",
    "                    tempFrame.remove(frame[j])\n",
    "            frame = tempFrame\n",
    "\n",
    "            neighbouring_tokens_dict = dict_[tokens[i]]\n",
    "            for k,index in enumerate(frame):\n",
    "                if tokens[i] == tokens[index]:\n",
    "                    continue\n",
    "                if tokens[index] not in neighbouring_tokens_dict :\n",
    "                    neighbouring_tokens_dict[tokens[index]] = 0\n",
    "                neighbouring_tokens_dict[tokens[index]] += 1 + 1/abs(tokens.index(tokens[i]) - tokens.index(tokens[index]))\n",
    "\n",
    "    c = Counter(words)\n",
    "    words_to_display = c.most_common(5000)\n",
    "    \n",
    "    w = []\n",
    "    for word in words_to_display :\n",
    "        w.append(word[0])\n",
    "\n",
    "    d = dict_.copy()\n",
    "\n",
    "    for key in dict_ :\n",
    "        if key not in w :\n",
    "            d.pop(key)\n",
    "\n",
    "    vector = [(k, v) for k, v in d.items()]\n",
    "\n",
    "    df1 = pd.DataFrame(vector)\n",
    "    df2 = pd.json_normalize(df1[1])\n",
    "\n",
    "    df2 = df2[df1[0]].fillna(0).set_index(df1[0])\n",
    "    \n",
    "    return df2\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "result = build_distance_matrix(trainingSet)\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "osW2EZ5utFsf"
   },
   "source": [
    "**c)**\tVous devez cr√©er une fonction `pmi` qui prend le cadre panda de la matrice $M(w,w)$ et un param√®tre boolean flag qui est √† True lorsque l'on d√©sire calculer PPMI et √† False quand on veut calculer PMI. La fonction transforme la matrice en entr√©e en une matrice $M‚Äô(w,w)$ avec les valeurs PMI ou PPMI selon la valeur du param√®tre bool√©en. La fonction retourne le nouveau cadre panda correspondant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "lksK36hntUHP"
   },
   "source": [
    "Pour une matrice  $X_{m \\times n}$:\n",
    "\n",
    "\n",
    "$$\\textbf{colsum}(X, j) = \\sum_{i=1}^{m}X_{ij}$$\n",
    "\n",
    "$$\\textbf{sum}(X) = \\sum_{i=1}^{m}\\sum_{j=1}^{n} X_{ij}$$\n",
    "\n",
    "$$\\textbf{expected}(X, i, j) = \n",
    "\\frac{\n",
    "  \\textbf{rowsum}(X, i) \\cdot \\textbf{colsum}(X, j)\n",
    "}{\n",
    "  \\textbf{sum}(X)\n",
    "}$$\n",
    "\n",
    "\n",
    "$$\\textbf{pmi}(X, i, j) = \\log\\left(\\frac{X_{ij}}{\\textbf{expected}(X, i, j)}\\right)$$\n",
    "\n",
    "$$\\textbf{ppmi}(X, i, j) = \n",
    "\\begin{cases}\n",
    "\\textbf{pmi}(X, i, j) & \\textrm{if } \\textbf{pmi}(X, i, j) > 0 \\\\\n",
    "0 & \\textrm{otherwise}\n",
    "\\end{cases}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "hidden": true,
    "id": "OgkN4mw6r19a"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def pmi(df, flag=True):\n",
    "    totalSum = 0\n",
    "    colsum = []\n",
    "    for column in df:\n",
    "        columnSum = df[column].sum()\n",
    "        totalSum += columnSum\n",
    "        colsum.append (columnSum)\n",
    "\n",
    "    rowsum = []\n",
    "    for row in df.sum(axis=1):\n",
    "        rowsum.append(row)\n",
    "    \n",
    "    expected = []\n",
    "    for row in rowsum:\n",
    "        eachRow = []\n",
    "        for col in colsum:\n",
    "            eachRow.append((row*col)/totalSum)\n",
    "        expected.append(eachRow)\n",
    "    \n",
    "    expected = np.array(expected)\n",
    "    dfArray = np.array(df.to_numpy())\n",
    "    dfArray = np.array(dfArray ,dtype = float)\n",
    "    valueInLog = np.divide(dfArray, expected, out=np.zeros_like(expected), where=dfArray!=0)\n",
    "    pmi = np.log2(valueInLog, out=np.zeros_like(valueInLog), where=valueInLog!=0)\n",
    "    \n",
    "    if flag:\n",
    "        pmi[pmi < 0] = 0\n",
    "    \n",
    "    return pd.DataFrame(pmi, index = df.index, columns = df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.178926229476929\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>gets</th>\n",
       "      <th>respect</th>\n",
       "      <th>sure</th>\n",
       "      <th>lot</th>\n",
       "      <th>memorable</th>\n",
       "      <th>quotes</th>\n",
       "      <th>listed</th>\n",
       "      <th>gem</th>\n",
       "      <th>imagine</th>\n",
       "      <th>...</th>\n",
       "      <th>macarthur</th>\n",
       "      <th>uwe</th>\n",
       "      <th>boll</th>\n",
       "      <th>seagal</th>\n",
       "      <th>porno</th>\n",
       "      <th>zizek</th>\n",
       "      <th>rambo</th>\n",
       "      <th>damme</th>\n",
       "      <th>prom</th>\n",
       "      <th>drivel</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>movie</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051947</td>\n",
       "      <td>0.349341</td>\n",
       "      <td>0.402087</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.448420</td>\n",
       "      <td>0.393894</td>\n",
       "      <td>0.170255</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.322964</td>\n",
       "      <td>0.069087</td>\n",
       "      <td>0.176375</td>\n",
       "      <td>0.657150</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gets</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.509233</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.149789</td>\n",
       "      <td>0.046632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188469</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.933267</td>\n",
       "      <td>0.338662</td>\n",
       "      <td>1.443376</td>\n",
       "      <td>1.800168</td>\n",
       "      <td>0.758229</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>respect</th>\n",
       "      <td>0.051947</td>\n",
       "      <td>0.509233</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.225142</td>\n",
       "      <td>0.931302</td>\n",
       "      <td>2.651132</td>\n",
       "      <td>2.355161</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.967638</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.401193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sure</th>\n",
       "      <td>0.349341</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.842317</td>\n",
       "      <td>0.240355</td>\n",
       "      <td>1.737792</td>\n",
       "      <td>1.856859</td>\n",
       "      <td>0.709223</td>\n",
       "      <td>0.463216</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.469336</td>\n",
       "      <td>0.126990</td>\n",
       "      <td>0.532385</td>\n",
       "      <td>1.052136</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lot</th>\n",
       "      <td>0.402087</td>\n",
       "      <td>0.149789</td>\n",
       "      <td>1.225142</td>\n",
       "      <td>0.842317</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.160443</td>\n",
       "      <td>0.279510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.523297</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.491088</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.476949</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.955035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zizek</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.338662</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.532385</td>\n",
       "      <td>0.955035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rambo</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.443376</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.052136</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>damme</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.800168</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.065928</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prom</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.758229</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.987653</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>drivel</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.401193</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows √ó 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            movie      gets   respect      sure       lot  memorable  \\\n",
       "0                                                                      \n",
       "movie    0.000000  0.000000  0.051947  0.349341  0.402087   0.000000   \n",
       "gets     0.000000  0.000000  0.509233  0.000000  0.149789   0.046632   \n",
       "respect  0.051947  0.509233  0.000000  0.000000  1.225142   0.931302   \n",
       "sure     0.349341  0.000000  0.000000  0.000000  0.842317   0.240355   \n",
       "lot      0.402087  0.149789  1.225142  0.842317  0.000000   0.000000   \n",
       "...           ...       ...       ...       ...       ...        ...   \n",
       "zizek    0.000000  0.338662  0.000000  0.532385  0.955035   0.000000   \n",
       "rambo    0.000000  1.443376  0.000000  1.052136  0.000000   0.000000   \n",
       "damme    0.000000  1.800168  0.000000  0.000000  0.000000   0.000000   \n",
       "prom     0.000000  0.758229  0.000000  0.000000  0.000000   1.987653   \n",
       "drivel   0.000000  0.000000  2.401193  0.000000  0.000000   0.000000   \n",
       "\n",
       "           quotes    listed       gem   imagine  ...  macarthur       uwe  \\\n",
       "0                                                ...                        \n",
       "movie    0.000000  0.448420  0.393894  0.170255  ...   0.000000  0.322964   \n",
       "gets     0.000000  0.000000  0.000000  0.000000  ...   0.188469  0.000000   \n",
       "respect  2.651132  2.355161  0.000000  0.000000  ...   0.000000  0.000000   \n",
       "sure     1.737792  1.856859  0.709223  0.463216  ...   0.000000  0.000000   \n",
       "lot      1.160443  0.279510  0.000000  0.523297  ...   0.000000  0.491088   \n",
       "...           ...       ...       ...       ...  ...        ...       ...   \n",
       "zizek    0.000000  0.000000  0.000000  0.000000  ...   0.000000  0.000000   \n",
       "rambo    0.000000  0.000000  0.000000  0.000000  ...   0.000000  0.000000   \n",
       "damme    0.000000  0.000000  0.000000  0.000000  ...   0.000000  0.000000   \n",
       "prom     0.000000  0.000000  0.000000  0.000000  ...   0.000000  0.000000   \n",
       "drivel   0.000000  0.000000  0.000000  0.000000  ...   0.000000  0.000000   \n",
       "\n",
       "             boll    seagal     porno     zizek     rambo     damme      prom  \\\n",
       "0                                                                               \n",
       "movie    0.069087  0.176375  0.657150  0.000000  0.000000  0.000000  0.000000   \n",
       "gets     0.000000  0.000000  0.933267  0.338662  1.443376  1.800168  0.758229   \n",
       "respect  0.000000  1.967638  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "sure     0.000000  0.469336  0.126990  0.532385  1.052136  0.000000  0.000000   \n",
       "lot      0.000000  0.476949  0.000000  0.955035  0.000000  0.000000  0.000000   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "zizek    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "rambo    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "damme    0.000000  6.065928  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "prom     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "drivel   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "           drivel  \n",
       "0                  \n",
       "movie    0.000000  \n",
       "gets     0.000000  \n",
       "respect  2.401193  \n",
       "sure     0.000000  \n",
       "lot      0.000000  \n",
       "...           ...  \n",
       "zizek    0.000000  \n",
       "rambo    0.000000  \n",
       "damme    0.000000  \n",
       "prom     0.000000  \n",
       "drivel   0.000000  \n",
       "\n",
       "[5000 rows x 5000 columns]"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "pmi_matrix = pmi(result)\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "pmi_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "U_KI11vmty0c"
   },
   "source": [
    "**d)** Cr√©er les matrice PMIs et PPMIs en vous basant sur les deux matrices que vous avez d√©j√† cr√©√©e\tSauvegardez vos matrices dans un fichier tp2_mat5<\\_scaled>_{pmi|ppmi}.csv toujours dans le r√©pertoire vsm. \n",
    "\n",
    "(votre nom de fichier doit contenir \"_scaled\" s'il est form√© √† partir Mww_scaled et \"pmi\" si le flag est false \"ppmi\" sinon) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "ML-oJwAgt0X4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "QwGaKZyZ8h36"
   },
   "source": [
    "## 3. Test de PPMI (20 points)\n",
    "\n",
    "Pour le test des matrices de cooccurrences, nous allons comparer deux mesures de distance entre deux vecteurs, la distance euclidienne et la distance cosinus provenant du module [scipy.spatial.distance](https://docs.scipy.org/doc/scipy/reference/spatial.distance.html)\n",
    "\n",
    "**Distance Euclidienne**\n",
    "\n",
    "La distance euclidienne entre deux vecteurs $u$ et $v$ de dimension $n$ est\n",
    "\n",
    "$$\\textbf{euclidean}(u, v) = \n",
    "\\sqrt{\\sum_{i=1}^{n}|u_{i} - v_{i}|^{2}}$$\n",
    "\n",
    "En deux dimensions, cela correspond √† la longueur de la ligne droite entre deux points.\n",
    "\n",
    "**Distance Cosinus**\n",
    "\n",
    "\n",
    "La distance cosinus entre deux vecteurs $u$ et $v$ de dimension $n$ s'√©crit :\n",
    "\n",
    "$$\\textbf{cosine}(u, v) = \n",
    "1 - \\frac{\\sum_{i=1}^{n} u_{i} \\cdot v_{i}}{\\|u\\|_{2} \\cdot \\|v\\|_{2}}$$\n",
    "\n",
    "Le terme de droite dans la soustraction mesure l'angle entre $u$ et $v$; on l'appelle la *similarit√© cosinus* entre $u$ et $v$.\n",
    "\n",
    "\\\\\n",
    "\n",
    "**a)**\tImpl√©mentez la fonction voisins(mot, pd, distance) qui prend un mot en entr√©e et une m√©trique de distance et qui retourne les n mots les plus similaires selon la mesure. Pour un mot w, elle ordonne tous les mots du vocabulaire en fonction de leur distance de w en utilisant la m√©trique de distance distance (par d√©faut: cosine)sur le vsm pd. Les mesures de distance √† tester sont : la distance Euclidienne et la distance cosinus implant√©es ci-dessus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "hidden": true,
    "id": "y6WzHmQ08eNG"
   },
   "outputs": [],
   "source": [
    "def voisins(word, df, distfunc, n):\n",
    "    distances = words = {}\n",
    "    similary_words = []\n",
    "    if(word not in voc): \n",
    "        return \"The word is not in list\"\n",
    "    else:\n",
    "        for i,word in enumerate(pmi_matrix.values):\n",
    "            if distfunc == 'euclidean':\n",
    "                distances[vocabulary[i]] = euclidean(df,word)\n",
    "            elif distfunc == 'cosine':\n",
    "                distances[vocabulary[i]] = cosine(df,word)\n",
    "\n",
    "        common_words = Counter(words).most_common(n) # Si c des tuples recup que le mot \n",
    "        for w in common_words:\n",
    "            similary_words.append(w[0])\n",
    "        \n",
    "    return similary_words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "ulI3R22m-AQq"
   },
   "source": [
    "**b)** En utilisant le cadre panda associ√© aux matrices Mww et Mww scaled, trouvez les 5 mots les plus similaires au mot ¬´ beautiful ¬ª et affichez-les, pour chacune des deux distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "hidden": true,
    "id": "7YqjqTYNMOQv",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  TODO : CHANGER LA MATRICE : MWW\n",
    "#print(\"EN UTILISANT LA MATRICE MOT PAR MOT : \")\n",
    "# print(\"Les 5 mots les plus similaires √† beautiful en utilisant la distance euclidienne est : \") \n",
    "# print(voisins('beautiful', result.values[vocabulary.index('beautiful')] , 'euclidean', 5))\n",
    "# print(\"Les 5 mots les plus similaires √† beautiful en utilisant la distance cosinus est : \")\n",
    "# print(voisins('beautiful', result.values[vocabulary.index('beautiful')] , 'cosine', 5))\n",
    "\n",
    "\n",
    "#  TODO : CHANGER LA MATRICE : MWW_SCALED\n",
    "#print(\"EN UTILISANT LA MATRICE MOT PAR MOT NORMALIS√âE : \")\n",
    "# print(\"Les 5 mots les plus similaires √† beautiful en utilisant la distance euclidienne est : \") \n",
    "# print(voisins('beautiful', result.values[vocabulary.index('beautiful')] , 'euclidean', 5))\n",
    "# print(\"Les 5 mots les plus similaires √† beautiful en utilisant la distance cosinus est : \")\n",
    "# print(voisins('beautiful', result.values[vocabulary.index('beautiful')] , 'cosine', 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "0_yvpfqIBL1a"
   },
   "source": [
    "**c)** En utilisant les cadres panda associ√©s aux matrices PMIs, trouvez les 5 mots les plus similaires au mot ¬´ beautiful ¬ª et affichez-les, pour chacune des deux distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "hidden": true,
    "id": "EHL_A0uEMPzO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN UTILISANT LA MATRICE PMI : \n",
      "Les 5 mots les plus similaires √† beautiful en utilisant la distance euclidienne est : \n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 615 is out of bounds for axis 0 with size 11",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-232-0d4fb84cc3cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"EN UTILISANT LA MATRICE PMI : \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Les 5 mots les plus similaires √† beautiful en utilisant la distance euclidienne est : \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoisins\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'beautiful'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpmi_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'beautiful'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m'euclidean'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Les 5 mots les plus similaires √† beautiful en utilisant la distance cosinus est : \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoisins\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'beautiful'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpmi_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'beautiful'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m'cosine'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 615 is out of bounds for axis 0 with size 11"
     ]
    }
   ],
   "source": [
    "print(\"EN UTILISANT LA MATRICE PMI : \")\n",
    "print(\"Les 5 mots les plus similaires √† beautiful en utilisant la distance euclidienne est : \") \n",
    "print(voisins('beautiful', pmi_matrix.values[vocabulary.index('beautiful')] , 'euclidean', 5))\n",
    "print(\"Les 5 mots les plus similaires √† beautiful en utilisant la distance cosinus est : \")\n",
    "print(voisins('beautiful', pmi_matrix.values[vocabulary.index('beautiful')] , 'cosine', 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "nW9kJOsUBSFf"
   },
   "source": [
    "**d)** En utilisant les cadres panda associ√©s aux matrices PPMIs, trouvez les 5 mots les plus similaires au mot\n",
    "¬´ beautiful ¬ª et affichez-les, pour chacune des deux distances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "oBb5xv9PMQ9y"
   },
   "outputs": [],
   "source": [
    "# # TODO : CHANGER LA MATRICE : PPMI\n",
    "print(\"EN UTILISANT LA MATRICE PPMI : \")\n",
    "# print(\"Les 5 mots les plus similaires √† beautiful en utilisant la distance euclidienne est : \") \n",
    "# print(voisins('beautiful', ppmi_matrix.values[vocabulary.index('beautiful')] , 'euclidean', 5))\n",
    "# print(\"Les 5 mots les plus similaires √† beautiful en utilisant la distance cosinus est : \")\n",
    "# print(voisins('beautiful', ppmi_matrix.values[vocabulary.index('beautiful')] , 'cosine', 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "B-Q3RfJXzdF8"
   },
   "source": [
    "**e)** Que constatez-vous entre la diff√©rence de performance de la distance euclidienne et la distance cosinus ? Que constatez-vous entre les diff√©rents types de matrices de cooccurrence ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "DcgcU9OtBg5-"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "q_COF24sBi5N"
   },
   "source": [
    "## 4.\tR√©duction de dimensionnalit√© (20 points)\n",
    "\n",
    "**a)** Ecrivez une fonction lsa qui prend en entr√©e un cadre panda pd (qui contient votre matrice / vsm) et un param√®tre K (qui indique le nombre de dimensions finales), et qui applique LSA avec ce param√®tre k sur la matrice et retourne le vsm r√©duit sous forme de cadre panda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "T6EjxKAQAvFw"
   },
   "outputs": [],
   "source": [
    "def lsa(df, k=100):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "pZDhlR2ZBwag"
   },
   "source": [
    "**b)** Ex√©cutez lsa sur les cadres panda associ√©s √† vos matrices Mww et Mww_scaled avec une dimension k=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "4Fh1yGAQBvsg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "0IQvZbddB4Zp"
   },
   "source": [
    "**c)** En utilisant les matrices de co-occurrence (de base et scal√©s) r√©duites avec LSA, trouvez les 5 mots les plus similaires au mot ¬´ beautiful ¬ª selon la distance cosinus et affichez-les."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "4BPts630Mw3t"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "y_wxbYFPCCCN"
   },
   "source": [
    "d) En utilisant les matrices PMIs et PPMIs r√©duites avec lsa, trouvez les 5 mots les plus similaires au mot ¬´ beautiful ¬ª selon la distance cosinus et affichez-les"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "QqgCh0WNM1Jl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "Qfwq16JtCSFy"
   },
   "source": [
    "**e)** En utilisant sklearn.decomposition.TruncatedSVD, cr√©ez les matrices r√©duites √† partir des m√™mes matrices que celles de la question pr√©c√©dentes (la matrice pmi et la matrice pmi_scaled) Puis tester ces nouvelles matrices LSA pour trouver les 5 mots les plus similaires au mot ¬´ beautiful ¬ª \n",
    "\n",
    "Ici aussi, nous voulons aussi obtenir des matrices de dimension k=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "Mj-aOpSSM2_O"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "9i7di2PRCTnh"
   },
   "source": [
    "f) Commentez vos r√©sultats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "ubED1oiwNJpm"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "K3avEFcPG0-e"
   },
   "source": [
    "## 5. √âvaluation (10 points)\n",
    "\n",
    "Il est temps d‚Äô√©valuer l‚Äôint√©r√™t de nos mod√®les de vecteurs. Nous allons pour cela utiliser un ensemble de donn√©es de similarit√© de mots (relatedness) The MEN Test Collection, qui se trouve dans le r√©pertoire test. L‚Äôensemble de donn√©es contient une paire de mots avec un score de similarit√© attribu√© par des humains. En d‚Äôautres termes, une ligne (un exemple) de l‚Äôensemble de donn√©es est de la forme : \\<mot_1> \\<mot_2> \\<score>.\n",
    "\n",
    "Pour aligner les distances obtenues avec vos m√©triques, ce score est converti en nombre r√©el n√©gatif par la fonction read_test_dataset que vous avez dans le squelette du TP.\n",
    "\n",
    "La m√©trique d‚Äô√©valuation est le coefficient de corr√©lation de Spearman ùúå entre les scores humains et vos distances (voir https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient). \n",
    "\n",
    "Nous allons maintenant √©valuer les diff√©rents vsm obtenus sur l'ensemble de donn√©es: MEN_dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "7j5LjMlav4pQ"
   },
   "source": [
    "#### Fonctions pour lire le jeu de donn√©es MEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "NodeMtrwGw0O"
   },
   "outputs": [],
   "source": [
    "def read_test_dataset(\n",
    "        src_filename,\n",
    "        delimiter=','):\n",
    "    with open(src_filename) as f:\n",
    "        reader = csv.reader(f, delimiter=delimiter)\n",
    "        for row in reader:\n",
    "            w1 = row[0].strip().lower()\n",
    "            w2 = row[1].strip().lower()\n",
    "            score = row[2]\n",
    "            score = -float(score)\n",
    "            yield (w1, w2, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "sTmkM5EDG-pA"
   },
   "outputs": [],
   "source": [
    "# Retourne un it√©rable sur le jeu de donn√©es MEN\n",
    "def men_dataset():\n",
    "    src_filename = os.path.join(\n",
    "        DIRNAME_MEN, 'MEN_dataset_natural_form_full')\n",
    "    return read_test_dataset(\n",
    "        src_filename, delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "uPd__Q4NHUlp"
   },
   "outputs": [],
   "source": [
    "def evaluate(ds, df, distfunc=cosine):\n",
    "    \"\"\"\n",
    "    ds : iterator\n",
    "       retourne des tuples (word1, word2, score).\n",
    "\n",
    "    df : pd.DataFrame\n",
    "        le mod√®le vsm √† √©valuer\n",
    "\n",
    "    distfunc : la mesure de distance entre vecteurs\n",
    "  \n",
    "    Retour: le coefficient de correlation de Spearman entre les scores de l'ensemble de donn√©es de test \n",
    "    et celui du modele vsm qui se pr√©sente sous la forme d'un cadre Panda pd avec les colonnes\n",
    "    ['word1', 'word2', 'score', 'distance'].\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for w1, w2, score in ds():\n",
    "        d = {'word1': w1, 'word2': w2,'score': score}\n",
    "        if w1 not in df.index or w2 not in df.index:\n",
    "            continue\n",
    "        else:\n",
    "            w1 = df.loc[w1]\n",
    "            w2 = df.loc[w2] \n",
    "        d['distance'] = distfunc(w1, w2)\n",
    "        data.append(d)\n",
    "\n",
    "    data = pd.DataFrame(data)\n",
    "    rho, pvalue = spearmanr(data['score'].values, b=data['distance'].values)\n",
    "    return rho, data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "MBrTrq8BHdLs"
   },
   "source": [
    "**a)**\tTestez chacun de vos mod√®les vsm (Matrice de base, matrice scal√©e, les PMIs et PPMIs et toutes les matrices LSA (de base, scal√©e, pmi, ppmi) en appelant la fonction evaluate avec les deux mesure de distance (euclidienne et cosinus) et affichez vos r√©sultats dans une seule table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "UL5s4dT4QiX_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "JjJwYKsTKXCR"
   },
   "source": [
    "**b)**\tCommentez vos r√©sultats d'√©valuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "-ZQhdrxFQjsb"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "inf8460_tp2_A20.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
