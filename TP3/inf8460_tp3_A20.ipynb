{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# École Polytechnique de Montréal\n",
    "# Département Génie Informatique et Génie Logiciel\n",
    "\n",
    "# INF8460 – Traitement automatique de la langue naturelle - TP3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectifs d’apprentissage\n",
    " • Utiliser des plongements lexicaux pré-entrainés pour de la classification\n",
    " \n",
    " • Entrainer des plongements lexicaux de type word2vec\n",
    " \n",
    " • Implanter des modèles de classification neuronaux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Équipe et contributions \n",
    "Veuillez indiquer la contribution effective de chaque membre de l'équipe en pourcentage et en indiquant les modules ou questions sur lesquelles chaque membre a travaillé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Nom Étudiant 1: x% (détail)\n",
    "\n",
    "Nom Étudiant 2: x% (détail)\n",
    "\n",
    "Nom Étudiant 3: x% (détail)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librairies externes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T13:30:14.696418Z",
     "start_time": "2020-09-24T13:30:14.651596Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import io\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sklearn\n",
    "import sklearn.naive_bayes\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from typing import Dict\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Téléchargement et lecture des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T13:06:47.074618Z",
     "start_time": "2020-09-24T13:06:47.026757Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join(os.getcwd(), \"aclImdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Téléchargement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T15:19:34.239196Z",
     "start_time": "2020-09-22T15:16:55.591044Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-09-22 11:16:55--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
      "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 84125825 (80M) [application/x-gzip]\n",
      "Saving to: ‘aclImdb_v1.tar.gz’\n",
      "\n",
      "aclImdb_v1.tar.gz   100%[===================>]  80.23M   378KB/s    in 2m 31s  \n",
      "\n",
      "2020-09-22 11:19:27 (544 KB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xzf aclImdb_v1.tar.gz\n",
    "!rm aclImdb_v1.tar.gz\n",
    "!echo Done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def download_wikipedia_embeddings() -> None:\n",
    "    if not os.path.exists(os.path.join(os.getcwd(), \"model.txt\")):\n",
    "        res = requests.get(\"http://vectors.nlpl.eu/repository/11/3.zip\")\n",
    "        with zipfile.ZipFile(io.BytesIO(res.content)) as z:\n",
    "            z.extractall(\"./\")\n",
    "        os.remove(os.path.join(os.getcwd(), \"3.zip\"))\n",
    "        os.remove(os.path.join(os.getcwd(), \"meta.json\"))\n",
    "        os.remove(os.path.join(os.getcwd(), \"model.bin\"))\n",
    "        os.remove(os.path.join(os.getcwd(), \"README\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T13:06:48.847418Z",
     "start_time": "2020-09-24T13:06:48.818869Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    traintest = ['train', 'test']\n",
    "    classes = ['pos', 'neg']\n",
    "    corpus = {cls: [] for cls in classes}\n",
    "\n",
    "    # Each data is a list of strings(reviews)\n",
    "    reviews = []\n",
    "    labels = []\n",
    "    for cls in classes:\n",
    "        dir_path = os.path.join(path, cls)\n",
    "        \n",
    "        for filename in os.listdir(dir_path):\n",
    "            file = os.path.join(dir_path, filename)\n",
    "            with open(file, encoding = 'utf-8') as f:\n",
    "                corpus[cls].append(f.read().replace(\"\\n\", \" \"))\n",
    "        \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T13:06:51.435025Z",
     "start_time": "2020-09-24T13:06:50.020587Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_data = read_data(os.path.join(DATA_PATH, 'train'))\n",
    "test_data = read_data(os.path.join(DATA_PATH, 'test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T13:06:51.480512Z",
     "start_time": "2020-09-24T13:06:51.437150Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_wikipedia_embeddings(word_indices: Dict[str, int], vocab_len: int) -> np.ndarray:\n",
    "    with open(\"./model.txt\", \"r\", encoding=\"UTF-8\") as f:\n",
    "        shape_string = f.readline()\n",
    "        lines = f.readlines() \n",
    "        \n",
    "    embedding = np.zeros((vocab_len, 300), dtype=float)\n",
    "    for line in lines:\n",
    "        splitted_line = line.split(\" \")\n",
    "        word = splitted_line[0].split(\"_\")[0]\n",
    "        if word in word_indices and word_indices[word] < vocab_len:\n",
    "            embedding_line = splitted_line[1:]\n",
    "            embedding[word_indices[word]] = list(map(float, embedding_line))\n",
    "        \n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Prétraitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T13:06:54.237924Z",
     "start_time": "2020-09-24T13:06:54.204609Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Preprocess(object):\n",
    "    def __init__(self, lemmatize=True):\n",
    "        self.stopwords = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "        self.lemmatize = lemmatize\n",
    "\n",
    "    def preprocess_pipeline(self, data):\n",
    "        clean_tokenized_data = self._clean_doc(data)\n",
    "        if self.lemmatize:\n",
    "            clean_tokenized_data = self._lemmatize(clean_tokenized_data)\n",
    "\n",
    "        return clean_tokenized_data\n",
    "\n",
    "    def _clean_doc(self, data):\n",
    "        tokenizer = nltk.tokenize.RegexpTokenizer(r\"\\w+\")\n",
    "        return [\n",
    "            [\n",
    "                token.lower()\n",
    "                for token in tokenizer.tokenize(review)\n",
    "                if token.lower() not in self.stopwords\n",
    "                and len(token) > 1\n",
    "                and token.isalpha()\n",
    "                and token != \"br]\"\n",
    "            ]\n",
    "            for review in data\n",
    "        ]\n",
    "\n",
    "    def _lemmatize(self, data):\n",
    "        lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "        return [[lemmatizer.lemmatize(word) for word in review] for review in data]\n",
    "\n",
    "    def convert_to_reviews(self, tokenized_reviews):\n",
    "        reviews = []\n",
    "        for tokens in tokenized_reviews:\n",
    "            reviews.append(\" \".join(tokens))\n",
    "\n",
    "        return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T13:07:29.745222Z",
     "start_time": "2020-09-24T13:06:55.097985Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 training sentences: 12500 pos and 12500 neg\n",
      "25000 test sentences: 12500 pos and 12500 neg\n"
     ]
    }
   ],
   "source": [
    "pre = Preprocess()\n",
    "\n",
    "train_pos = pre.preprocess_pipeline(train_data[\"pos\"])\n",
    "train_neg = pre.preprocess_pipeline(train_data[\"neg\"])\n",
    "test_pos = pre.preprocess_pipeline(test_data[\"pos\"])\n",
    "test_neg = pre.preprocess_pipeline(test_data[\"neg\"])\n",
    "\n",
    "y_train = [1] * len(train_pos) + [0] * len(train_neg)\n",
    "y_test = [1] * len(test_pos) + [0] * len(test_neg)\n",
    "X_train = [\" \".join(sentence) for sentence in train_pos + train_neg]\n",
    "X_test = [\" \".join(sentence) for sentence in test_pos + test_neg]\n",
    "\n",
    "print(\"{} training sentences: {} pos and {} neg\".format(len(X_train), len(train_pos), len(train_neg)))\n",
    "print(\"{} test sentences: {} pos and {} neg\".format(len(X_test), len(test_pos), len(test_neg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Entrainement de plongements lexicaux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous devez réaliser les étapes suivantes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Utiliser Gensim pour entrainer un modèle word2vec sur le corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Décrire les paramètres du ou des modèles entraînés, leur taille sur disque, le nombre de mots encodés, le temps d'entraînement, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Décrire le cas échéant et de manière précise tout problème que vous avez eu à obtenir votre modèle et les façons de résoudre ces problèmes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) Retrouvez les 5 mots voisins des mots suivants : excellent, terrible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Classification avec des plongements lexicaux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On vous demande d’effectuer de la classification avec les plongements lexicaux obtenus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) En reprenant le code développé dans le TP1 avec Scikitlearn, on vous demande cette fois de tester un modèle Naïve Bayes et de régression logistique avec des n-grammes (n=1,2,3 ensemble). Essayez de voir si une réduction de dimension améliore la classification. Ne fournissez que votre meilleur modèle. Evaluez vos algorithmes selon les métriques d’accuracy générale et de F1 par classe sur l’ensemble de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) En utilisant Tensorflow (ou Pytorch), on vous demande de développer un classificateur perceptron multicouches et un bi-LSTM avec les vecteurs d’un modèle word2vec pré-entrainé sur Wikipédia en Anglais (enwiki_upos_skipgram_300_3_2019) disponible à http://vectors.nlpl.eu/repository/11/3.zip. \n",
    "\n",
    "On s’attend à ce que vous effectuiez une moyenne des vecteurs de mots de chaque document pour obtenir un plongement du document.  \n",
    "\n",
    "Evaluez vos algorithmes selon les métriques d’accuracy générale et de F1 par classe sur l’ensemble de test. Pour chacun des modèles, indiquez ses performances et ses spécifications (nombre d’époques, régularisation, optimiseur, nombre de couches, etc.). N’hésitez pas à expérimenter avec différents paramètres. Vous ne devez reporter que votre meilleure expérimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Ré-entrainez les modèles en b) avec vos propres vecteurs. Comparez maintenant la performance obtenue en en b) avec celles que vous obtenez en utilisant vos propres vecteurs de mots entrainés sur le corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) Générez une table ou un graphique qui regroupe les performances des modèles, leurs spécifications, la durée d’entraînement et commentez ces résultats. Quelle est l’influence des word embeddings sur les performances?  Quel est votre meilleur modèle ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
